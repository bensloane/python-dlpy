{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Learning Rate Policy</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is one of the most important hyperparameter to train for you neural network in order to achieve good performance. In the tutorial, you will learn how to specify predefined learning rate policy and customize your own learning rate with FCMP function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import swat\n",
    "import sys\n",
    "import dlpy\n",
    "from dlpy.layers import *\n",
    "from dlpy.model import *\n",
    "from dlpy.images import ImageTable\n",
    "from dlpy.sequential import Sequential\n",
    "from dlpy.lr_scheduler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_name='your_host_name'\n",
    "port_number='your_port_number'\n",
    "sess = swat.CAS(host_name, port_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build network\n",
    "First, Let's build a simple ResNet like model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, size, stride=1, mode='same', act=True):\n",
    "    x = Conv2d(filters, size, size, act='identity', include_bias=False, stride=stride)(x)\n",
    "    x = BN(act='relu' if act else 'identity')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, 2)\n",
    "    x = conv_block(x, nf, 3, 1, act=False)\n",
    "    return Res()([x, ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "inp_resnet= Input(3, 112, 112, scale = 1.0 / 255, name='InputLayer_1')\n",
    "x=conv_block(inp_resnet, 64, 9, 1)\n",
    "for i in range(4): x=res_block(x)\n",
    "x=Conv2d(20, 9, 9, act='tanh')(x)\n",
    "x=Pooling(7, 7)(x)\n",
    "output = OutputLayer(n=2)(x)\n",
    "resnet_like_model = Model(sess, inputs = inp_resnet, outputs = output)\n",
    "resnet_like_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Model_N3w4tJ Pages: 1 -->\n",
       "<svg width=\"319pt\" height=\"1781pt\"\n",
       " viewBox=\"0.00 0.00 319.00 1781.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1777)\">\n",
       "<title>Model_N3w4tJ</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1777 315,-1777 315,4 -4,4\"/>\n",
       "<!-- InputLayer_1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>InputLayer_1</title>\n",
       "<polygon fill=\"#3288bd\" fill-opacity=\"0.250980\" stroke=\"#3288bd\" points=\"78.5,-1750.5 78.5,-1772.5 293.5,-1772.5 293.5,-1750.5 78.5,-1750.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-1757.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">112x112x3 InputLayer_1(input)</text>\n",
       "</g>\n",
       "<!-- Conv2d_1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Conv2d_1</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"105.5,-1680.5 105.5,-1702.5 266.5,-1702.5 266.5,-1680.5 105.5,-1680.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-1687.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">9x9 Conv2d_1(convo)</text>\n",
       "</g>\n",
       "<!-- InputLayer_1&#45;&gt;Conv2d_1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>InputLayer_1&#45;&gt;Conv2d_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M186,-1750.2533C186,-1740.3113 186,-1725.5277 186,-1713.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"189.5001,-1712.9513 186,-1702.9513 182.5001,-1712.9514 189.5001,-1712.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"219\" y=\"-1724\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 112 x 112 x 3 </text>\n",
       "</g>\n",
       "<!-- BN_1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>BN_1</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"79.5,-1610.5 79.5,-1632.5 292.5,-1632.5 292.5,-1610.5 79.5,-1610.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-1617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">112x112x64 BN_1(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_1&#45;&gt;BN_1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Conv2d_1&#45;&gt;BN_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M186,-1680.2533C186,-1670.3113 186,-1655.5277 186,-1643.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"189.5001,-1642.9513 186,-1632.9513 182.5001,-1642.9514 189.5001,-1642.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.5\" y=\"-1654\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 112 x 112 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Conv2d_2</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"46.5,-1540.5 46.5,-1562.5 207.5,-1562.5 207.5,-1540.5 46.5,-1540.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-1547.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_2(convo)</text>\n",
       "</g>\n",
       "<!-- BN_1&#45;&gt;Conv2d_2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>BN_1&#45;&gt;Conv2d_2</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M173.0484,-1610.2808C167.2544,-1604.9904 160.5049,-1598.4365 155,-1592 149.4996,-1585.5688 144.066,-1578.0549 139.4701,-1571.2575\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"142.2047,-1569.045 133.7946,-1562.6014 136.3508,-1572.8832 142.2047,-1569.045\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.5\" y=\"-1584\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 112 x 112 x 64 </text>\n",
       "</g>\n",
       "<!-- Res_1 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>Res_1</title>\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"84,-1260.5 84,-1282.5 270,-1282.5 270,-1260.5 84,-1260.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"177\" y=\"-1267.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">56x56x64 Res_1(residual)</text>\n",
       "</g>\n",
       "<!-- BN_1&#45;&gt;Res_1 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>BN_1&#45;&gt;Res_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M205.9363,-1610.2962C213.0851,-1605.4537 220.6835,-1599.2295 226,-1592 237.283,-1576.657 240,-1570.5451 240,-1551.5 240,-1551.5 240,-1551.5 240,-1341.5 240,-1319.2233 222.1543,-1300.8305 205.4588,-1288.4977\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"207.1515,-1285.4132 196.9336,-1282.6123 203.1746,-1291.1738 207.1515,-1285.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.5\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 112 x 112 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>BN_2</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"14,-1470.5 14,-1492.5 212,-1492.5 212,-1470.5 14,-1470.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-1477.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">56x56x64 BN_2(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_2&#45;&gt;BN_2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Conv2d_2&#45;&gt;BN_2</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M124.7507,-1540.2533C122.742,-1530.2098 119.7451,-1515.2255 117.2807,-1502.9034\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"120.6835,-1502.0707 115.2903,-1492.9513 113.8195,-1503.4436 120.6835,-1502.0707\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-1514\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Conv2d_3</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"32.5,-1400.5 32.5,-1422.5 193.5,-1422.5 193.5,-1400.5 32.5,-1400.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-1407.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_3(convo)</text>\n",
       "</g>\n",
       "<!-- BN_2&#45;&gt;Conv2d_3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>BN_2&#45;&gt;Conv2d_3</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M113,-1470.2533C113,-1460.3113 113,-1445.5277 113,-1433.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"116.5001,-1432.9513 113,-1422.9513 109.5001,-1432.9514 116.5001,-1432.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.5\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_3 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>BN_3</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"14,-1330.5 14,-1352.5 212,-1352.5 212,-1330.5 14,-1330.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-1337.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">56x56x64 BN_3(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_3&#45;&gt;BN_3 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Conv2d_3&#45;&gt;BN_3</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M113,-1400.2533C113,-1390.3113 113,-1375.5277 113,-1363.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"116.5001,-1362.9513 113,-1352.9513 109.5001,-1362.9514 116.5001,-1362.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.5\" y=\"-1374\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_3&#45;&gt;Res_1 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>BN_3&#45;&gt;Res_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M123.2827,-1330.2533C133.1146,-1319.4997 148.1252,-1303.0818 159.7819,-1290.3323\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"162.3656,-1292.6934 166.5302,-1282.9513 157.1994,-1287.97 162.3656,-1292.6934\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-1304\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_4 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Conv2d_4</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"39.5,-1190.5 39.5,-1212.5 200.5,-1212.5 200.5,-1190.5 39.5,-1190.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"120\" y=\"-1197.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_4(convo)</text>\n",
       "</g>\n",
       "<!-- Res_1&#45;&gt;Conv2d_4 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>Res_1&#45;&gt;Conv2d_4</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M167.842,-1260.2533C159.2507,-1249.7026 146.2195,-1233.6993 135.9256,-1221.0578\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"138.353,-1218.4957 129.3247,-1212.9513 132.9249,-1222.9157 138.353,-1218.4957\"/>\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-1234\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- Res_2 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>Res_2</title>\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"77,-910.5 77,-932.5 263,-932.5 263,-910.5 77,-910.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"170\" y=\"-917.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">28x28x64 Res_2(residual)</text>\n",
       "</g>\n",
       "<!-- Res_1&#45;&gt;Res_2 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>Res_1&#45;&gt;Res_2</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M196.3609,-1260.4511C203.6061,-1255.5528 211.4226,-1249.2492 217,-1242 228.8017,-1226.6609 233,-1220.8538 233,-1201.5 233,-1201.5 233,-1201.5 233,-991.5 233,-969.2233 215.1543,-950.8305 198.4588,-938.4977\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"200.1515,-935.4132 189.9336,-932.6123 196.1746,-941.1738 200.1515,-935.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"263.5\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 56 x 56 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_4 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>BN_4</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"7,-1120.5 7,-1142.5 205,-1142.5 205,-1120.5 7,-1120.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-1127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">28x28x64 BN_4(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_4&#45;&gt;BN_4 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>Conv2d_4&#45;&gt;BN_4</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M117.7507,-1190.2533C115.742,-1180.2098 112.7451,-1165.2255 110.2807,-1152.9034\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"113.6835,-1152.0707 108.2903,-1142.9513 106.8195,-1153.4436 113.6835,-1152.0707\"/>\n",
       "<text text-anchor=\"middle\" x=\"144.5\" y=\"-1164\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_5 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>Conv2d_5</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"25.5,-1050.5 25.5,-1072.5 186.5,-1072.5 186.5,-1050.5 25.5,-1050.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-1057.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_5(convo)</text>\n",
       "</g>\n",
       "<!-- BN_4&#45;&gt;Conv2d_5 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>BN_4&#45;&gt;Conv2d_5</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M106,-1120.2533C106,-1110.3113 106,-1095.5277 106,-1083.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"109.5001,-1082.9513 106,-1072.9513 102.5001,-1082.9514 109.5001,-1082.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.5\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_5 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>BN_5</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"7,-980.5 7,-1002.5 205,-1002.5 205,-980.5 7,-980.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-987.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">28x28x64 BN_5(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_5&#45;&gt;BN_5 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>Conv2d_5&#45;&gt;BN_5</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M106,-1050.2533C106,-1040.3113 106,-1025.5277 106,-1013.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"109.5001,-1012.9513 106,-1002.9513 102.5001,-1012.9514 109.5001,-1012.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"136.5\" y=\"-1024\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_5&#45;&gt;Res_2 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>BN_5&#45;&gt;Res_2</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M116.2827,-980.2533C126.1146,-969.4997 141.1252,-953.0818 152.7819,-940.3323\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"155.3656,-942.6934 159.5302,-932.9513 150.1994,-937.97 155.3656,-942.6934\"/>\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-954\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_6 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>Conv2d_6</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"32.5,-840.5 32.5,-862.5 193.5,-862.5 193.5,-840.5 32.5,-840.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-847.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_6(convo)</text>\n",
       "</g>\n",
       "<!-- Res_2&#45;&gt;Conv2d_6 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>Res_2&#45;&gt;Conv2d_6</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M160.842,-910.2533C152.2507,-899.7026 139.2195,-883.6993 128.9256,-871.0578\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"131.353,-868.4957 122.3247,-862.9513 125.9249,-872.9157 131.353,-868.4957\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-884\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- Res_3 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>Res_3</title>\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"70,-560.5 70,-582.5 256,-582.5 256,-560.5 70,-560.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"163\" y=\"-567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">14x14x64 Res_3(residual)</text>\n",
       "</g>\n",
       "<!-- Res_2&#45;&gt;Res_3 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>Res_2&#45;&gt;Res_3</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M189.3609,-910.4511C196.6061,-905.5528 204.4226,-899.2492 210,-892 221.8017,-876.6609 226,-870.8538 226,-851.5 226,-851.5 226,-851.5 226,-641.5 226,-619.2233 208.1543,-600.8305 191.4588,-588.4977\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"193.1515,-585.4132 182.9336,-582.6123 189.1746,-591.1738 193.1515,-585.4132\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.5\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 28 x 28 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_6 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>BN_6</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-770.5 0,-792.5 198,-792.5 198,-770.5 0,-770.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-777.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">14x14x64 BN_6(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_6&#45;&gt;BN_6 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>Conv2d_6&#45;&gt;BN_6</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M110.7507,-840.2533C108.742,-830.2098 105.7451,-815.2255 103.2807,-802.9034\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"106.6835,-802.0707 101.2903,-792.9513 99.8195,-803.4436 106.6835,-802.0707\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-814\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_7 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>Conv2d_7</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"18.5,-700.5 18.5,-722.5 179.5,-722.5 179.5,-700.5 18.5,-700.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-707.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_7(convo)</text>\n",
       "</g>\n",
       "<!-- BN_6&#45;&gt;Conv2d_7 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>BN_6&#45;&gt;Conv2d_7</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M99,-770.2533C99,-760.3113 99,-745.5277 99,-733.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"102.5001,-732.9513 99,-722.9513 95.5001,-732.9514 102.5001,-732.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.5\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_7 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>BN_7</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-630.5 0,-652.5 198,-652.5 198,-630.5 0,-630.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-637.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">14x14x64 BN_7(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_7&#45;&gt;BN_7 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>Conv2d_7&#45;&gt;BN_7</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M99,-700.2533C99,-690.3113 99,-675.5277 99,-663.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"102.5001,-662.9513 99,-652.9513 95.5001,-662.9514 102.5001,-662.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"129.5\" y=\"-674\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_7&#45;&gt;Res_3 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>BN_7&#45;&gt;Res_3</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M109.2827,-630.2533C119.1146,-619.4997 134.1252,-603.0818 145.7819,-590.3323\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"148.3656,-592.6934 152.5302,-582.9513 143.1994,-587.97 148.3656,-592.6934\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.5\" y=\"-604\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_8 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>Conv2d_8</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"26.5,-490.5 26.5,-512.5 187.5,-512.5 187.5,-490.5 26.5,-490.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-497.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_8(convo)</text>\n",
       "</g>\n",
       "<!-- Res_3&#45;&gt;Conv2d_8 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>Res_3&#45;&gt;Conv2d_8</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M154.0026,-560.2533C145.5621,-549.7026 132.7595,-533.6993 122.6462,-521.0578\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"125.1411,-518.5736 116.1611,-512.9513 119.675,-522.9465 125.1411,-518.5736\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-534\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- Res_4 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>Res_4</title>\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"73.5,-210.5 73.5,-232.5 244.5,-232.5 244.5,-210.5 73.5,-210.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-217.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">7x7x64 Res_4(residual)</text>\n",
       "</g>\n",
       "<!-- Res_3&#45;&gt;Res_4 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>Res_3&#45;&gt;Res_4</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M183.3539,-560.3234C190.6716,-555.4842 198.4743,-549.2552 204,-542 215.6302,-526.7297 219,-520.6949 219,-501.5 219,-501.5 219,-501.5 219,-291.5 219,-269.5553 201.791,-251.0385 185.8056,-238.5721\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"187.795,-235.691 177.6555,-232.6184 183.6658,-241.3434 187.795,-235.691\"/>\n",
       "<text text-anchor=\"middle\" x=\"249.5\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 14 x 14 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_8 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>BN_8</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"7.5,-420.5 7.5,-442.5 190.5,-442.5 190.5,-420.5 7.5,-420.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-427.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">7x7x64 BN_8(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_8&#45;&gt;BN_8 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>Conv2d_8&#45;&gt;BN_8</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M105.7147,-490.2533C104.5668,-480.2098 102.8543,-465.2255 101.4461,-452.9034\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"104.9216,-452.4892 100.3087,-442.9513 97.9669,-453.2841 104.9216,-452.4892\"/>\n",
       "<text text-anchor=\"middle\" x=\"129\" y=\"-464\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_9 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>Conv2d_9</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"18.5,-350.5 18.5,-372.5 179.5,-372.5 179.5,-350.5 18.5,-350.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-357.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">3x3 Conv2d_9(convo)</text>\n",
       "</g>\n",
       "<!-- BN_8&#45;&gt;Conv2d_9 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>BN_8&#45;&gt;Conv2d_9</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M99,-420.2533C99,-410.3113 99,-395.5277 99,-383.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"102.5001,-382.9513 99,-372.9513 95.5001,-382.9514 102.5001,-382.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"124\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_9 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>BN_9</title>\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"7.5,-280.5 7.5,-302.5 190.5,-302.5 190.5,-280.5 7.5,-280.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-287.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">7x7x64 BN_9(batchnorm)</text>\n",
       "</g>\n",
       "<!-- Conv2d_9&#45;&gt;BN_9 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>Conv2d_9&#45;&gt;BN_9</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M99,-350.2533C99,-340.3113 99,-325.5277 99,-313.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"102.5001,-312.9513 99,-302.9513 95.5001,-312.9514 102.5001,-312.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"124\" y=\"-324\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 64 </text>\n",
       "</g>\n",
       "<!-- BN_9&#45;&gt;Res_4 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>BN_9&#45;&gt;Res_4</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M108.64,-280.2533C117.7704,-269.6011 131.6648,-253.3911 142.5479,-240.6941\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"145.334,-242.8217 149.1846,-232.9513 140.0192,-238.2662 145.334,-242.8217\"/>\n",
       "<text text-anchor=\"middle\" x=\"158\" y=\"-254\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 64 </text>\n",
       "</g>\n",
       "<!-- Conv2d_10 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>Conv2d_10</title>\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"75,-140.5 75,-162.5 243,-162.5 243,-140.5 75,-140.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-147.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">9x9 Conv2d_10(convo)</text>\n",
       "</g>\n",
       "<!-- Res_4&#45;&gt;Conv2d_10 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>Res_4&#45;&gt;Conv2d_10</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M159,-210.2533C159,-200.3113 159,-185.5277 159,-173.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"162.5001,-172.9513 159,-162.9513 155.5001,-172.9514 162.5001,-172.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 64 </text>\n",
       "</g>\n",
       "<!-- Pooling_1 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>Pooling_1</title>\n",
       "<polygon fill=\"#66c2a5\" fill-opacity=\"0.250980\" stroke=\"#66c2a5\" points=\"84.5,-70.5 84.5,-92.5 233.5,-92.5 233.5,-70.5 84.5,-70.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-77.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">7x7 Pooling_1(pool)</text>\n",
       "</g>\n",
       "<!-- Conv2d_10&#45;&gt;Pooling_1 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>Conv2d_10&#45;&gt;Pooling_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M159,-140.2533C159,-130.3113 159,-115.5277 159,-103.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"162.5001,-102.9513 159,-92.9513 155.5001,-102.9514 162.5001,-102.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-114\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 7 x 7 x 20 </text>\n",
       "</g>\n",
       "<!-- OutputLayer_1 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>OutputLayer_1</title>\n",
       "<polygon fill=\"#5e4fa2\" fill-opacity=\"0.125490\" stroke=\"#5e4fa2\" points=\"60,-.5 60,-22.5 258,-22.5 258,-.5 60,-.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">20x2 OutputLayer_1(output)</text>\n",
       "</g>\n",
       "<!-- Pooling_1&#45;&gt;OutputLayer_1 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>Pooling_1&#45;&gt;OutputLayer_1</title>\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M159,-70.2533C159,-60.3113 159,-45.5277 159,-33.2776\"/>\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"162.5001,-32.9513 159,-22.9513 155.5001,-32.9514 162.5001,-32.9513\"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-44\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\" fill=\"#000000\"> 1 x 1 x 20 </text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f3cdf9f2668>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.plot_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path='/disk/linux/dlpy/Giraffe_Dolphin'\n",
    "my_images = ImageTable.load_files(sess, path=img_path)\n",
    "my_images.resize(112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLPy have some predefined learning rate policies.\n",
    "1. FixedLR\n",
    "2. StepLR\n",
    "3. MultiStepLR\n",
    "4. PolynomialLR\n",
    "5. ReduceLROnPlateau\n",
    "6. CyclicLR\n",
    "\n",
    "Besides, you can also customize your own learning rate policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = StepLR(learning_rate=0.0001, gamma=0.1, step_size=2)\n",
    "solver = MomentumSolver(lr_scheduler=lr_scheduler, clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=5, reg_l2=0.0005)\n",
    "gpu = Gpu(devices=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training from scratch.\n",
      "WARNING: Failed to load TKGPU extension.\n",
      "NOTE: Using dlgrd008.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       7.13 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8923     0.4531     0.2989     0.31\n",
      "NOTE:      1    64   0.0001           0.7282     0.3438     0.2989     0.04\n",
      "NOTE:      2    64   0.0001           0.7244     0.3438     0.2989     0.04\n",
      "NOTE:      3    64   0.0001           0.8218     0.4063     0.2989     0.04\n",
      "NOTE:      4    64   0.0001           0.8228     0.4063     0.2989     0.04\n",
      "NOTE:      5    64   0.0001           0.7957     0.3906     0.2989     0.04\n",
      "NOTE:      6    64   0.0001           0.7243     0.3438     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0001          0.7871     0.3839     0.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.8897     0.4531     0.2989     0.05\n",
      "NOTE:      1    64   0.0001           0.8143     0.4063     0.2989     0.04\n",
      "NOTE:      2    64   0.0001           0.7178     0.3438     0.2989     0.04\n",
      "NOTE:      3    64   0.0001           0.8144     0.4063     0.2989     0.04\n",
      "NOTE:      4    64   0.0001           0.8781     0.4531     0.2989     0.04\n",
      "NOTE:      5    64   0.0001           0.8797     0.4531     0.2989     0.04\n",
      "NOTE:      6    64   0.0001           0.9224     0.4844     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0001          0.8452     0.4286     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001           0.8293     0.4219     0.2989     0.05\n",
      "NOTE:      1    64  0.00001           0.8277     0.4219     0.2989     0.04\n",
      "NOTE:      2    64  0.00001            0.913     0.4844     0.2989     0.04\n",
      "NOTE:      3    64  0.00001           0.9357        0.5     0.2989     0.04\n",
      "NOTE:      4    64  0.00001           0.7981     0.4063     0.2989     0.04\n",
      "NOTE:      5    64  0.00001           0.7763     0.3906     0.2989     0.04\n",
      "NOTE:      6    64  0.00001           0.8894     0.4688     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          1E-5          0.8528      0.442     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001            0.864     0.4531     0.2989     0.05\n",
      "NOTE:      1    64  0.00001            0.932        0.5     0.2989     0.04\n",
      "NOTE:      2    64  0.00001           0.8633     0.4531     0.2989     0.04\n",
      "NOTE:      3    64  0.00001            0.838     0.4375     0.2989     0.04\n",
      "NOTE:      4    64  0.00001           0.8619     0.4531     0.2989     0.04\n",
      "NOTE:      5    64  0.00001           0.7498      0.375     0.2989     0.04\n",
      "NOTE:      6    64  0.00001           0.8371     0.4375     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          1E-5          0.8495     0.4442     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.7915     0.4063     0.2989     0.05\n",
      "NOTE:      1    64     1E-6           0.7918     0.4063     0.2989     0.04\n",
      "NOTE:      2    64     1E-6           0.8363     0.4375     0.2989     0.04\n",
      "NOTE:      3    64     1E-6           0.7923     0.4063     0.2989     0.04\n",
      "NOTE:      4    64     1E-6           0.8126     0.4219     0.2989     0.04\n",
      "NOTE:      5    64     1E-6           0.9872     0.5469     0.2989     0.04\n",
      "NOTE:      6    64     1E-6           0.7269     0.3594     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          1E-6          0.8198     0.4263     0.31\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       1.79 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_n3w4tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.787074</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.298871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.845204</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.298867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.852782</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.298863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.849453</td>\n",
       "      <td>0.444196</td>\n",
       "      <td>0.298860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.819807</td>\n",
       "      <td>0.426339</td>\n",
       "      <td>0.298859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(ethem-kinginthenorth)</td>\n",
       "      <td>Model_N3w4tJ_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_N3w4tJ_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 9.45s</span> &#183; <span class=\"cas-user\">user 2.62s</span> &#183; <span class=\"cas-sys\">sys 1.27s</span> &#183; <span class=\"cas-memory\">mem 130MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_n3w4tj\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "    Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      1      0.000100  0.787074  0.383929  0.298871\n",
       " 1      2      0.000100  0.845204  0.428571  0.298867\n",
       " 2      3      0.000010  0.852782  0.441964  0.298863\n",
       " 3      4      0.000010  0.849453  0.444196  0.298860\n",
       " 4      5      0.000001  0.819807  0.426339  0.298859\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(ethem-kinginthenorth)  Model_N3w4tJ_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_N3w4tJ_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 9.45s, user: 2.62s, sys: 1.27s, mem: 130mb"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyclic Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = CyclicLR(conn=sess, data=my_images, max_lr=0.01, batch_size=1, factor=2,\n",
    "                        learning_rate=0.0001)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "WARNING: Failed to load TKGPU extension.\n",
      "NOTE: Using dlgrd008.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       7.14 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            1.071     0.6094     0.2989     0.31\n",
      "NOTE:      1    64 0.000112           0.8131     0.4219     0.2989     0.04\n",
      "NOTE:      2    64 0.000124           0.7933     0.4063     0.2989     0.04\n",
      "NOTE:      3    64 0.000136           0.7451      0.375     0.2989     0.04\n",
      "NOTE:      4    64 0.000148           0.7859     0.4063     0.2989     0.04\n",
      "NOTE:      5    64 0.000161           0.8923     0.4844     0.2989     0.04\n",
      "NOTE:      6    64 0.000173           0.7653     0.3906     0.2989     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0002           0.838      0.442     0.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505             0.74      0.375     0.2989     0.05\n",
      "NOTE:      1    64 0.005062           0.8873     0.4844     0.2989     0.04\n",
      "NOTE:      2    64 0.005074           0.7691     0.4063     0.2988     0.04\n",
      "NOTE:      3    64 0.005086           0.8782     0.5156     0.2988     0.04\n",
      "NOTE:      4    64 0.005098            0.771     0.4531     0.2988     0.04\n",
      "NOTE:      5    64 0.005111           0.7374     0.4531     0.2988     0.04\n",
      "NOTE:      6    64 0.005123           0.7061     0.4531     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0051          0.7841     0.4487     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.7245     0.5469     0.2988     0.05\n",
      "NOTE:      1    64 0.009988           0.6743        0.5     0.2988     0.04\n",
      "NOTE:      2    64 0.009976           0.6358     0.1719     0.2987     0.04\n",
      "NOTE:      3    64 0.009964            0.652     0.4844     0.2987     0.04\n",
      "NOTE:      4    64 0.009952            0.607     0.3594     0.2987     0.04\n",
      "NOTE:      5    64 0.009939            0.501       0.25     0.2987     0.04\n",
      "NOTE:      6    64 0.009927            0.466     0.3594     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2        0.0099          0.6087     0.3817     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.3663     0.2031     0.2986     0.05\n",
      "NOTE:      1    64 0.005038           0.3736     0.1563     0.2986     0.04\n",
      "NOTE:      2    64 0.005026           0.2599    0.03125     0.2986     0.04\n",
      "NOTE:      3    64 0.005014           0.2159    0.01563     0.2986     0.04\n",
      "NOTE:      4    64 0.005002           0.1978    0.03125     0.2986     0.04\n",
      "NOTE:      5    64 0.004989           0.1598    0.01563     0.2986     0.04\n",
      "NOTE:      6    64 0.004977           0.2316    0.07813     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3         0.005          0.2578    0.07589     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.1372    0.03125     0.2986     0.05\n",
      "NOTE:      1    64 0.000112          0.09662    0.03125     0.2986     0.04\n",
      "NOTE:      2    64 0.000124           0.1108    0.03125     0.2986     0.04\n",
      "NOTE:      3    64 0.000136           0.1506    0.04688     0.2986     0.04\n",
      "NOTE:      4    64 0.000148          0.06117          0     0.2986     0.04\n",
      "NOTE:      5    64 0.000161           0.1435    0.04688     0.2986     0.04\n",
      "NOTE:      6    64 0.000173           0.1316    0.04688     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4        0.0002          0.1188    0.03348     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.04533          0     0.2986     0.05\n",
      "NOTE:      1    64 0.005062           0.1372     0.0625     0.2986     0.04\n",
      "NOTE:      2    64 0.005074           0.1352    0.03125     0.2986     0.04\n",
      "NOTE:      3    64 0.005086           0.1813    0.09375     0.2986     0.04\n",
      "NOTE:      4    64 0.005098          0.08598    0.01563     0.2986     0.04\n",
      "NOTE:      5    64 0.005111          0.05415    0.01563     0.2986     0.04\n",
      "NOTE:      6    64 0.005123          0.07477    0.03125     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5        0.0051           0.102    0.03571     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.08516    0.01563     0.2986     0.05\n",
      "NOTE:      1    64 0.009988          0.05044    0.01563     0.2986     0.04\n",
      "NOTE:      2    64 0.009976          0.09878    0.01563     0.2985     0.04\n",
      "NOTE:      3    64 0.009964           0.1251    0.07813     0.2985     0.04\n",
      "NOTE:      4    64 0.009952          0.08902    0.01563     0.2985     0.04\n",
      "NOTE:      5    64 0.009939          0.09355    0.01563     0.2985     0.04\n",
      "NOTE:      6    64 0.009927          0.08969    0.01563     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6        0.0099         0.09024    0.02455     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.05994    0.03125     0.2984     0.05\n",
      "NOTE:      1    64 0.005038          0.04616          0     0.2984     0.04\n",
      "NOTE:      2    64 0.005026            0.119    0.03125     0.2984     0.04\n",
      "NOTE:      3    64 0.005014           0.1139    0.04688     0.2984     0.04\n",
      "NOTE:      4    64 0.005002          0.03893    0.01563     0.2983     0.04\n",
      "NOTE:      5    64 0.004989          0.07437    0.03125     0.2983     0.04\n",
      "NOTE:      6    64 0.004977          0.07941    0.03125     0.2983     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.005         0.07596    0.02679     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.1467    0.04688     0.2983     0.05\n",
      "NOTE:      1    64 0.000112          0.03975          0     0.2982     0.04\n",
      "NOTE:      2    64 0.000124          0.05346    0.01563     0.2982     0.04\n",
      "NOTE:      3    64 0.000136          0.09825    0.04688     0.2982     0.04\n",
      "NOTE:      4    64 0.000148          0.04922    0.03125     0.2982     0.04\n",
      "NOTE:      5    64 0.000161          0.08068    0.01563     0.2982     0.04\n",
      "NOTE:      6    64 0.000173          0.04211          0     0.2981     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8        0.0002         0.07289    0.02232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.09295    0.04688     0.2981     0.05\n",
      "NOTE:      1    64 0.005062          0.09989    0.04688     0.2981     0.04\n",
      "NOTE:      2    64 0.005074          0.05894    0.03125     0.2981     0.04\n",
      "NOTE:      3    64 0.005086          0.06289    0.01563     0.2981     0.04\n",
      "NOTE:      4    64 0.005098          0.04467    0.01563     0.2981     0.04\n",
      "NOTE:      5    64 0.005111          0.04902    0.01563     0.2981     0.04\n",
      "NOTE:      6    64 0.005123          0.06603    0.03125      0.298     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9        0.0051         0.06777    0.02902     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.04214    0.01563      0.298     0.05\n",
      "NOTE:      1    64 0.009988          0.02468          0      0.298     0.04\n",
      "NOTE:      2    64 0.009976          0.01484          0      0.298     0.04\n",
      "NOTE:      3    64 0.009964          0.02716          0      0.298     0.04\n",
      "NOTE:      4    64 0.009952          0.08046    0.03125     0.2979     0.04\n",
      "NOTE:      5    64 0.009939          0.02566          0     0.2979     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      6    64 0.009927          0.02431          0     0.2979     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10       0.0099         0.03418   0.006696     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505            0.062    0.01563     0.2978     0.05\n",
      "NOTE:      1    64 0.005038          0.02959          0     0.2978     0.04\n",
      "NOTE:      2    64 0.005026          0.01593          0     0.2978     0.04\n",
      "NOTE:      3    64 0.005014          0.01677          0     0.2977     0.04\n",
      "NOTE:      4    64 0.005002           0.0483    0.01563     0.2977     0.04\n",
      "NOTE:      5    64 0.004989          0.03368    0.01563     0.2977     0.04\n",
      "NOTE:      6    64 0.004977          0.02789    0.01563     0.2976     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.005         0.03345   0.008929     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01706          0     0.2976     0.05\n",
      "NOTE:      1    64 0.000112          0.01725          0     0.2976     0.04\n",
      "NOTE:      2    64 0.000124          0.07894    0.01563     0.2975     0.04\n",
      "NOTE:      3    64 0.000136          0.01606          0     0.2975     0.04\n",
      "NOTE:      4    64 0.000148          0.01115          0     0.2975     0.04\n",
      "NOTE:      5    64 0.000161          0.07651    0.01563     0.2975     0.04\n",
      "NOTE:      6    64 0.000173          0.01186          0     0.2975     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0002         0.03269   0.004464     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.06611    0.03125     0.2974     0.05\n",
      "NOTE:      1    64 0.005062          0.04352    0.03125     0.2974     0.04\n",
      "NOTE:      2    64 0.005074          0.01032          0     0.2974     0.04\n",
      "NOTE:      3    64 0.005086          0.01505          0     0.2974     0.04\n",
      "NOTE:      4    64 0.005098          0.04327    0.01563     0.2974     0.04\n",
      "NOTE:      5    64 0.005111          0.04005    0.01563     0.2974     0.04\n",
      "NOTE:      6    64 0.005123          0.07812    0.01563     0.2973     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0051         0.04235    0.01563     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01153          0     0.2973     0.05\n",
      "NOTE:      1    64 0.009988          0.02879    0.01563     0.2973     0.04\n",
      "NOTE:      2    64 0.009976          0.01754          0     0.2973     0.04\n",
      "NOTE:      3    64 0.009964           0.1823     0.0625     0.2972     0.04\n",
      "NOTE:      4    64 0.009952          0.01242          0     0.2972     0.04\n",
      "NOTE:      5    64 0.009939          0.02199          0     0.2972     0.04\n",
      "NOTE:      6    64 0.009927          0.02849          0     0.2971     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0099          0.0433    0.01116     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.02666          0     0.2971     0.05\n",
      "NOTE:      1    64 0.005038          0.01839          0     0.2971     0.04\n",
      "NOTE:      2    64 0.005026           0.1991     0.0625      0.297     0.04\n",
      "NOTE:      3    64 0.005014          0.01119          0      0.297     0.04\n",
      "NOTE:      4    64 0.005002          0.01692          0      0.297     0.04\n",
      "NOTE:      5    64 0.004989          0.02506          0     0.2969     0.04\n",
      "NOTE:      6    64 0.004977          0.02014          0     0.2969     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15        0.005         0.04535   0.008929     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.07793    0.01563     0.2969     0.05\n",
      "NOTE:      1    64 0.000112          0.04076          0     0.2969     0.04\n",
      "NOTE:      2    64 0.000124          0.06421    0.03125     0.2968     0.04\n",
      "NOTE:      3    64 0.000136          0.02942          0     0.2968     0.04\n",
      "NOTE:      4    64 0.000148          0.04549    0.01563     0.2968     0.04\n",
      "NOTE:      5    64 0.000161          0.01663          0     0.2968     0.04\n",
      "NOTE:      6    64 0.000173          0.02553          0     0.2968     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0002         0.04285   0.008929     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.03082          0     0.2967     0.05\n",
      "NOTE:      1    64 0.005062          0.04186    0.03125     0.2967     0.04\n",
      "NOTE:      2    64 0.005074          0.05504    0.03125     0.2967     0.04\n",
      "NOTE:      3    64 0.005086           0.0588    0.01563     0.2967     0.04\n",
      "NOTE:      4    64 0.005098           0.0171          0     0.2967     0.04\n",
      "NOTE:      5    64 0.005111          0.06234    0.01563     0.2967     0.04\n",
      "NOTE:      6    64 0.005123          0.06531    0.03125     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0051         0.04732    0.01786     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.04413    0.01563     0.2966     0.05\n",
      "NOTE:      1    64 0.009988           0.0453    0.01563     0.2966     0.04\n",
      "NOTE:      2    64 0.009976          0.02439          0     0.2966     0.04\n",
      "NOTE:      3    64 0.009964          0.02595          0     0.2966     0.04\n",
      "NOTE:      4    64 0.009952          0.07226    0.03125     0.2965     0.04\n",
      "NOTE:      5    64 0.009939          0.03916    0.01563     0.2965     0.04\n",
      "NOTE:      6    64 0.009927           0.0197          0     0.2965     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0099          0.0387    0.01116     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01094          0     0.2965     0.05\n",
      "NOTE:      1    64 0.005038          0.02969    0.01563     0.2964     0.04\n",
      "NOTE:      2    64 0.005026         0.009048          0     0.2964     0.04\n",
      "NOTE:      3    64 0.005014         0.009141          0     0.2964     0.04\n",
      "NOTE:      4    64 0.005002          0.01999          0     0.2963     0.04\n",
      "NOTE:      5    64 0.004989          0.03328    0.01563     0.2963     0.04\n",
      "NOTE:      6    64 0.004977          0.01365          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19        0.005         0.01796   0.004464     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.008471          0     0.2962     0.05\n",
      "NOTE:      1    64 0.000112         0.007079          0     0.2962     0.04\n",
      "NOTE:      2    64 0.000124          0.01006          0     0.2962     0.04\n",
      "NOTE:      3    64 0.000136           0.1475    0.07813     0.2962     0.04\n",
      "NOTE:      4    64 0.000148         0.007056          0     0.2961     0.04\n",
      "NOTE:      5    64 0.000161          0.03795    0.01563     0.2961     0.04\n",
      "NOTE:      6    64 0.000173          0.01767          0     0.2961     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0002         0.03368    0.01339     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.006189          0     0.2961     0.05\n",
      "NOTE:      1    64 0.005062          0.02132          0     0.2961     0.04\n",
      "NOTE:      2    64 0.005074          0.02407          0     0.2961     0.04\n",
      "NOTE:      3    64 0.005086          0.01161          0      0.296     0.04\n",
      "NOTE:      4    64 0.005098          0.09294    0.04688      0.296     0.04\n",
      "NOTE:      5    64 0.005111         0.005538          0      0.296     0.04\n",
      "NOTE:      6    64 0.005123          0.01278          0      0.296     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0051         0.02492   0.006696     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01761          0      0.296     0.05\n",
      "NOTE:      1    64 0.009988         0.006625          0     0.2959     0.04\n",
      "NOTE:      2    64 0.009976          0.01389          0     0.2959     0.04\n",
      "NOTE:      3    64 0.009964          0.02676    0.01563     0.2959     0.04\n",
      "NOTE:      4    64 0.009952           0.0228    0.01563     0.2959     0.04\n",
      "NOTE:      5    64 0.009939          0.01182          0     0.2958     0.04\n",
      "NOTE:      6    64 0.009927         0.007327          0     0.2958     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0099         0.01526   0.004464     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01041          0     0.2958     0.05\n",
      "NOTE:      1    64 0.005038           0.0111          0     0.2957     0.04\n",
      "NOTE:      2    64 0.005026          0.05592    0.01563     0.2957     0.04\n",
      "NOTE:      3    64 0.005014         0.007154          0     0.2957     0.04\n",
      "NOTE:      4    64 0.005002          0.01048          0     0.2956     0.04\n",
      "NOTE:      5    64 0.004989         0.006406          0     0.2956     0.04\n",
      "NOTE:      6    64 0.004977          0.01174          0     0.2956     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  23        0.005         0.01617   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.006863          0     0.2955     0.05\n",
      "NOTE:      1    64 0.000112         0.006835          0     0.2955     0.04\n",
      "NOTE:      2    64 0.000124         0.006705          0     0.2955     0.04\n",
      "NOTE:      3    64 0.000136         0.007391          0     0.2954     0.04\n",
      "NOTE:      4    64 0.000148          0.03386    0.01563     0.2954     0.04\n",
      "NOTE:      5    64 0.000161         0.007023          0     0.2954     0.04\n",
      "NOTE:      6    64 0.000173         0.007988          0     0.2954     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24       0.0002         0.01095   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.008802          0     0.2954     0.05\n",
      "NOTE:      1    64 0.005062          0.01121          0     0.2953     0.04\n",
      "NOTE:      2    64 0.005074          0.01411          0     0.2953     0.04\n",
      "NOTE:      3    64 0.005086          0.01472          0     0.2953     0.04\n",
      "NOTE:      4    64 0.005098          0.02929    0.01563     0.2953     0.04\n",
      "NOTE:      5    64 0.005111          0.01399          0     0.2953     0.04\n",
      "NOTE:      6    64 0.005123         0.007359          0     0.2952     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25       0.0051         0.01421   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01166          0     0.2952     0.05\n",
      "NOTE:      1    64 0.009988         0.005861          0     0.2952     0.04\n",
      "NOTE:      2    64 0.009976          0.00942          0     0.2952     0.04\n",
      "NOTE:      3    64 0.009964         0.009931          0     0.2952     0.04\n",
      "NOTE:      4    64 0.009952         0.007322          0     0.2951     0.04\n",
      "NOTE:      5    64 0.009939         0.004858          0     0.2951     0.04\n",
      "NOTE:      6    64 0.009927         0.005254          0     0.2951     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26       0.0099        0.007758          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.004849          0      0.295     0.05\n",
      "NOTE:      1    64 0.005038          0.01228          0      0.295     0.04\n",
      "NOTE:      2    64 0.005026         0.005098          0     0.2949     0.04\n",
      "NOTE:      3    64 0.005014         0.006911          0     0.2949     0.04\n",
      "NOTE:      4    64 0.005002         0.005094          0     0.2949     0.04\n",
      "NOTE:      5    64 0.004989         0.004603          0     0.2948     0.04\n",
      "NOTE:      6    64 0.004977         0.005409          0     0.2948     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27        0.005         0.00632          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004644          0     0.2948     0.05\n",
      "NOTE:      1    64 0.000112         0.006467          0     0.2947     0.04\n",
      "NOTE:      2    64 0.000124         0.004155          0     0.2947     0.04\n",
      "NOTE:      3    64 0.000136         0.005041          0     0.2947     0.04\n",
      "NOTE:      4    64 0.000148         0.004001          0     0.2947     0.04\n",
      "NOTE:      5    64 0.000161         0.009616          0     0.2946     0.04\n",
      "NOTE:      6    64 0.000173         0.007115          0     0.2946     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28       0.0002        0.005863          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.007531          0     0.2946     0.05\n",
      "NOTE:      1    64 0.005062         0.008537          0     0.2946     0.04\n",
      "NOTE:      2    64 0.005074         0.005878          0     0.2946     0.04\n",
      "NOTE:      3    64 0.005086         0.007979          0     0.2945     0.04\n",
      "NOTE:      4    64 0.005098          0.00509          0     0.2945     0.04\n",
      "NOTE:      5    64 0.005111         0.005974          0     0.2945     0.04\n",
      "NOTE:      6    64 0.005123         0.004969          0     0.2945     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29       0.0051        0.006565          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.005756          0     0.2945     0.05\n",
      "NOTE:      1    64 0.009988         0.008858          0     0.2944     0.04\n",
      "NOTE:      2    64 0.009976         0.003555          0     0.2944     0.04\n",
      "NOTE:      3    64 0.009964         0.004936          0     0.2944     0.04\n",
      "NOTE:      4    64 0.009952         0.008579          0     0.2943     0.04\n",
      "NOTE:      5    64 0.009939         0.003542          0     0.2943     0.04\n",
      "NOTE:      6    64 0.009927          0.00668          0     0.2943     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30       0.0099        0.005986          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.006866          0     0.2942     0.05\n",
      "NOTE:      1    64 0.005038         0.006149          0     0.2942     0.04\n",
      "NOTE:      2    64 0.005026         0.005302          0     0.2942     0.04\n",
      "NOTE:      3    64 0.005014          0.01264          0     0.2941     0.04\n",
      "NOTE:      4    64 0.005002         0.005941          0     0.2941     0.04\n",
      "NOTE:      5    64 0.004989         0.005031          0      0.294     0.04\n",
      "NOTE:      6    64 0.004977         0.003712          0      0.294     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31        0.005         0.00652          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.008055          0      0.294     0.05\n",
      "NOTE:      1    64 0.000112         0.004269          0     0.2939     0.04\n",
      "NOTE:      2    64 0.000124         0.004003          0     0.2939     0.04\n",
      "NOTE:      3    64 0.000136         0.004778          0     0.2939     0.04\n",
      "NOTE:      4    64 0.000148         0.003216          0     0.2939     0.04\n",
      "NOTE:      5    64 0.000161         0.006626          0     0.2938     0.04\n",
      "NOTE:      6    64 0.000173         0.005102          0     0.2938     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32       0.0002         0.00515          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.003354          0     0.2938     0.05\n",
      "NOTE:      1    64 0.005062         0.005365          0     0.2938     0.04\n",
      "NOTE:      2    64 0.005074         0.004912          0     0.2938     0.04\n",
      "NOTE:      3    64 0.005086         0.004245          0     0.2937     0.04\n",
      "NOTE:      4    64 0.005098          0.00376          0     0.2937     0.04\n",
      "NOTE:      5    64 0.005111         0.003727          0     0.2937     0.04\n",
      "NOTE:      6    64 0.005123         0.004782          0     0.2937     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33       0.0051        0.004306          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003976          0     0.2937     0.05\n",
      "NOTE:      1    64 0.009988         0.004734          0     0.2936     0.04\n",
      "NOTE:      2    64 0.009976         0.003428          0     0.2936     0.04\n",
      "NOTE:      3    64 0.009964         0.003393          0     0.2936     0.04\n",
      "NOTE:      4    64 0.009952         0.003861          0     0.2936     0.04\n",
      "NOTE:      5    64 0.009939         0.004008          0     0.2935     0.04\n",
      "NOTE:      6    64 0.009927          0.00376          0     0.2935     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34       0.0099         0.00388          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.00774          0     0.2934     0.05\n",
      "NOTE:      1    64 0.005038         0.004972          0     0.2934     0.04\n",
      "NOTE:      2    64 0.005026         0.005807          0     0.2934     0.04\n",
      "NOTE:      3    64 0.005014         0.004584          0     0.2933     0.04\n",
      "NOTE:      4    64 0.005002           0.0049          0     0.2933     0.04\n",
      "NOTE:      5    64 0.004989         0.004562          0     0.2933     0.04\n",
      "NOTE:      6    64 0.004977         0.004136          0     0.2932     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35        0.005        0.005243          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.004161          0     0.2932     0.05\n",
      "NOTE:      1    64 0.000112         0.003423          0     0.2931     0.04\n",
      "NOTE:      2    64 0.000124         0.002833          0     0.2931     0.04\n",
      "NOTE:      3    64 0.000136         0.007715          0     0.2931     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64 0.000148          0.00353          0     0.2931     0.04\n",
      "NOTE:      5    64 0.000161         0.004546          0      0.293     0.04\n",
      "NOTE:      6    64 0.000173         0.002985          0      0.293     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36       0.0002        0.004171          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.009218          0      0.293     0.05\n",
      "NOTE:      1    64 0.005062         0.004612          0      0.293     0.04\n",
      "NOTE:      2    64 0.005074         0.003077          0      0.293     0.04\n",
      "NOTE:      3    64 0.005086           0.1056     0.0625      0.293     0.04\n",
      "NOTE:      4    64 0.005098         0.003125          0     0.2929     0.04\n",
      "NOTE:      5    64 0.005111         0.004161          0     0.2929     0.04\n",
      "NOTE:      6    64 0.005123           0.1274    0.04688     0.2929     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37       0.0051         0.03674    0.01563     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.006556          0     0.2929     0.05\n",
      "NOTE:      1    64 0.009988         0.007357          0     0.2928     0.04\n",
      "NOTE:      2    64 0.009976         0.003991          0     0.2928     0.04\n",
      "NOTE:      3    64 0.009964         0.004627          0     0.2928     0.04\n",
      "NOTE:      4    64 0.009952          0.01823          0     0.2928     0.04\n",
      "NOTE:      5    64 0.009939          0.03719    0.01563     0.2927     0.04\n",
      "NOTE:      6    64 0.009927         0.003494          0     0.2927     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38       0.0099         0.01163   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.005722          0     0.2927     0.05\n",
      "NOTE:      1    64 0.005038          0.00478          0     0.2926     0.04\n",
      "NOTE:      2    64 0.005026         0.005297          0     0.2926     0.04\n",
      "NOTE:      3    64 0.005014         0.002749          0     0.2926     0.04\n",
      "NOTE:      4    64 0.005002          0.01066          0     0.2925     0.04\n",
      "NOTE:      5    64 0.004989          0.02039          0     0.2925     0.04\n",
      "NOTE:      6    64 0.004977          0.02302    0.01563     0.2925     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39        0.005         0.01037   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.00437          0     0.2924     0.05\n",
      "NOTE:      1    64 0.000112          0.04952    0.03125     0.2924     0.04\n",
      "NOTE:      2    64 0.000124         0.004511          0     0.2924     0.04\n",
      "NOTE:      3    64 0.000136         0.004433          0     0.2924     0.04\n",
      "NOTE:      4    64 0.000148          0.01399          0     0.2923     0.04\n",
      "NOTE:      5    64 0.000161         0.005353          0     0.2923     0.04\n",
      "NOTE:      6    64 0.000173         0.004571          0     0.2923     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40       0.0002         0.01239   0.004464     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.008845          0     0.2923     0.05\n",
      "NOTE:      1    64 0.005062         0.005441          0     0.2923     0.04\n",
      "NOTE:      2    64 0.005074           0.0266    0.01563     0.2923     0.04\n",
      "NOTE:      3    64 0.005086         0.003332          0     0.2922     0.04\n",
      "NOTE:      4    64 0.005098          0.01734          0     0.2922     0.04\n",
      "NOTE:      5    64 0.005111             0.21    0.07813     0.2922     0.04\n",
      "NOTE:      6    64 0.005123         0.005082          0     0.2922     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41       0.0051         0.03953    0.01339     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003915          0     0.2922     0.05\n",
      "NOTE:      1    64 0.009988         0.003376          0     0.2921     0.04\n",
      "NOTE:      2    64 0.009976         0.004075          0     0.2921     0.04\n",
      "NOTE:      3    64 0.009964         0.005155          0     0.2921     0.04\n",
      "NOTE:      4    64 0.009952         0.006993          0     0.2921     0.04\n",
      "NOTE:      5    64 0.009939         0.004478          0      0.292     0.04\n",
      "NOTE:      6    64 0.009927         0.004094          0      0.292     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42       0.0099        0.004584          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.00429          0      0.292     0.05\n",
      "NOTE:      1    64 0.005038         0.008147          0     0.2919     0.04\n",
      "NOTE:      2    64 0.005026         0.005359          0     0.2919     0.04\n",
      "NOTE:      3    64 0.005014         0.006823          0     0.2918     0.04\n",
      "NOTE:      4    64 0.005002           0.0033          0     0.2918     0.04\n",
      "NOTE:      5    64 0.004989         0.007281          0     0.2918     0.04\n",
      "NOTE:      6    64 0.004977         0.003225          0     0.2917     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43        0.005        0.005489          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.0244          0     0.2917     0.05\n",
      "NOTE:      1    64 0.000112         0.005447          0     0.2917     0.04\n",
      "NOTE:      2    64 0.000124          0.00397          0     0.2916     0.04\n",
      "NOTE:      3    64 0.000136          0.00378          0     0.2916     0.04\n",
      "NOTE:      4    64 0.000148          0.00619          0     0.2916     0.04\n",
      "NOTE:      5    64 0.000161          0.02448    0.01563     0.2916     0.04\n",
      "NOTE:      6    64 0.000173          0.01573          0     0.2916     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44       0.0002           0.012   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.006942          0     0.2915     0.05\n",
      "NOTE:      1    64 0.005062         0.004431          0     0.2915     0.04\n",
      "NOTE:      2    64 0.005074         0.007403          0     0.2915     0.04\n",
      "NOTE:      3    64 0.005086         0.003002          0     0.2915     0.04\n",
      "NOTE:      4    64 0.005098         0.002614          0     0.2915     0.04\n",
      "NOTE:      5    64 0.005111         0.004573          0     0.2915     0.04\n",
      "NOTE:      6    64 0.005123         0.007411          0     0.2914     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45       0.0051        0.005197          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002369          0     0.2914     0.05\n",
      "NOTE:      1    64 0.009988         0.003116          0     0.2914     0.04\n",
      "NOTE:      2    64 0.009976         0.002862          0     0.2914     0.04\n",
      "NOTE:      3    64 0.009964           0.0237          0     0.2913     0.04\n",
      "NOTE:      4    64 0.009952         0.004825          0     0.2913     0.04\n",
      "NOTE:      5    64 0.009939         0.002389          0     0.2913     0.04\n",
      "NOTE:      6    64 0.009927          0.00373          0     0.2912     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46       0.0099        0.006142          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.003421          0     0.2912     0.05\n",
      "NOTE:      1    64 0.005038         0.002755          0     0.2912     0.04\n",
      "NOTE:      2    64 0.005026          0.00342          0     0.2911     0.04\n",
      "NOTE:      3    64 0.005014         0.006989          0     0.2911     0.04\n",
      "NOTE:      4    64 0.005002         0.002951          0      0.291     0.04\n",
      "NOTE:      5    64 0.004989         0.005111          0      0.291     0.04\n",
      "NOTE:      6    64 0.004977         0.004329          0      0.291     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47        0.005        0.004139          0     0.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002287          0     0.2909     0.05\n",
      "NOTE:      1    64 0.000112         0.003794          0     0.2909     0.04\n",
      "NOTE:      2    64 0.000124         0.006265          0     0.2909     0.04\n",
      "NOTE:      3    64 0.000136         0.003082          0     0.2909     0.04\n",
      "NOTE:      4    64 0.000148         0.003537          0     0.2908     0.04\n",
      "NOTE:      5    64 0.000161         0.003569          0     0.2908     0.04\n",
      "NOTE:      6    64 0.000173         0.004039          0     0.2908     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48       0.0002        0.003796          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.002818          0     0.2908     0.05\n",
      "NOTE:      1    64 0.005062          0.03863    0.03125     0.2908     0.04\n",
      "NOTE:      2    64 0.005074         0.003356          0     0.2907     0.04\n",
      "NOTE:      3    64 0.005086         0.002915          0     0.2907     0.04\n",
      "NOTE:      4    64 0.005098         0.007388          0     0.2907     0.04\n",
      "NOTE:      5    64 0.005111            0.043    0.03125     0.2907     0.04\n",
      "NOTE:      6    64 0.005123         0.005186          0     0.2907     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49       0.0051         0.01476   0.008929     0.30\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      15.46 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_n3w4tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.837965</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.298857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.784142</td>\n",
       "      <td>0.448661</td>\n",
       "      <td>0.298834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.608654</td>\n",
       "      <td>0.381696</td>\n",
       "      <td>0.298712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.257839</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>0.298576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.118782</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.298558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.101992</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.298578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.090244</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.298520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.075958</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.298354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.072885</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.298189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.067769</td>\n",
       "      <td>0.029018</td>\n",
       "      <td>0.298084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.297947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.033451</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.297725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.297521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.042347</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.297392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.043298</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.297241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.297012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.042852</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.296817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>23</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.047323</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.296702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>0.011161</td>\n",
       "      <td>0.296564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.017961</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.296356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.296163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.024922</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.296041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>28</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.015264</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.295893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.016173</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.010952</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.295307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.007758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>33</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>35</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.006565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>36</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>37</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.294122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>39</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>41</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>42</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.036738</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.292949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>44</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.011634</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.292792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>45</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.292565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>46</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.012393</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.292361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>47</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.292232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>48</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.292080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>49</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.291626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>51</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>52</td>\n",
       "      <td>0.009927</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>53</td>\n",
       "      <td>0.004977</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>54</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>55</td>\n",
       "      <td>0.005123</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.290716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(ethem-kinginthenorth)</td>\n",
       "      <td>Model_N3w4tJ_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_N3w4tJ_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 23.1s</span> &#183; <span class=\"cas-user\">user 19.5s</span> &#183; <span class=\"cas-sys\">sys 4.27s</span> &#183; <span class=\"cas-memory\">mem 131MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_n3w4tj\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       6      0.000173  0.837965  0.441964  0.298857\n",
       " 1       7      0.005123  0.784142  0.448661  0.298834\n",
       " 2       8      0.009927  0.608654  0.381696  0.298712\n",
       " 3       9      0.004977  0.257839  0.075893  0.298576\n",
       " 4      10      0.000173  0.118782  0.033482  0.298558\n",
       " 5      11      0.005123  0.101992  0.035714  0.298578\n",
       " 6      12      0.009927  0.090244  0.024554  0.298520\n",
       " 7      13      0.004977  0.075958  0.026786  0.298354\n",
       " 8      14      0.000173  0.072885  0.022321  0.298189\n",
       " 9      15      0.005123  0.067769  0.029018  0.298084\n",
       " 10     16      0.009927  0.034179  0.006696  0.297947\n",
       " 11     17      0.004977  0.033451  0.008929  0.297725\n",
       " 12     18      0.000173  0.032690  0.004464  0.297521\n",
       " 13     19      0.005123  0.042347  0.015625  0.297392\n",
       " 14     20      0.009927  0.043298  0.011161  0.297241\n",
       " 15     21      0.004977  0.045354  0.008929  0.297012\n",
       " 16     22      0.000173  0.042852  0.008929  0.296817\n",
       " 17     23      0.005123  0.047323  0.017857  0.296702\n",
       " 18     24      0.009927  0.038697  0.011161  0.296564\n",
       " 19     25      0.004977  0.017961  0.004464  0.296356\n",
       " 20     26      0.000173  0.033677  0.013393  0.296163\n",
       " 21     27      0.005123  0.024922  0.006696  0.296041\n",
       " 22     28      0.009927  0.015264  0.004464  0.295893\n",
       " 23     29      0.004977  0.016173  0.002232  0.295660\n",
       " 24     30      0.000173  0.010952  0.002232  0.295444\n",
       " 25     31      0.005123  0.014210  0.002232  0.295307\n",
       " 26     32      0.009927  0.007758  0.000000  0.295149\n",
       " 27     33      0.004977  0.006320  0.000000  0.294907\n",
       " 28     34      0.000173  0.005863  0.000000  0.294680\n",
       " 29     35      0.005123  0.006565  0.000000  0.294536\n",
       " 30     36      0.009927  0.005986  0.000000  0.294372\n",
       " 31     37      0.004977  0.006520  0.000000  0.294122\n",
       " 32     38      0.000173  0.005150  0.000000  0.293891\n",
       " 33     39      0.005123  0.004306  0.000000  0.293745\n",
       " 34     40      0.009927  0.003880  0.000000  0.293579\n",
       " 35     41      0.004977  0.005243  0.000000  0.293327\n",
       " 36     42      0.000173  0.004171  0.000000  0.293095\n",
       " 37     43      0.005123  0.036738  0.015625  0.292949\n",
       " 38     44      0.009927  0.011634  0.002232  0.292792\n",
       " 39     45      0.004977  0.010374  0.002232  0.292565\n",
       " 40     46      0.000173  0.012393  0.004464  0.292361\n",
       " 41     47      0.005123  0.039526  0.013393  0.292232\n",
       " 42     48      0.009927  0.004584  0.000000  0.292080\n",
       " 43     49      0.004977  0.005489  0.000000  0.291845\n",
       " 44     50      0.000173  0.012000  0.002232  0.291626\n",
       " 45     51      0.005123  0.005197  0.000000  0.291487\n",
       " 46     52      0.009927  0.006142  0.000000  0.291326\n",
       " 47     53      0.004977  0.004139  0.000000  0.291082\n",
       " 48     54      0.000173  0.003796  0.000000  0.290857\n",
       " 49     55      0.005123  0.014756  0.008929  0.290716\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(ethem-kinginthenorth)  Model_N3w4tJ_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_N3w4tJ_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 23.1s, user: 19.5s, sys: 4.27s, mem: 131mb"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Learning Rate on Plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(conn=sess, cool_down_iters=2, gamma=0.1, learning_rate=0.01, patience=3)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "WARNING: Failed to load TKGPU extension.\n",
      "NOTE: Using dlgrd008.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       6.93 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.00444          0     0.2906     0.31\n",
      "NOTE:      1    64     0.01         0.005897          0     0.2906     0.04\n",
      "NOTE:      2    64     0.01         0.004235          0     0.2906     0.04\n",
      "NOTE:      3    64     0.01          0.01166          0     0.2906     0.04\n",
      "NOTE:      4    64     0.01          0.00269          0     0.2906     0.04\n",
      "NOTE:      5    64     0.01         0.005041          0     0.2906     0.04\n",
      "NOTE:      6    64     0.01          0.09545    0.04688     0.2905     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01         0.01849   0.006696     0.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003612          0     0.2905     0.05\n",
      "NOTE:      1    64     0.01          0.01117          0     0.2905     0.04\n",
      "NOTE:      2    64     0.01           0.1436    0.04688     0.2905     0.04\n",
      "NOTE:      3    64     0.01         0.006186          0     0.2904     0.04\n",
      "NOTE:      4    64     0.01         0.003579          0     0.2904     0.04\n",
      "NOTE:      5    64     0.01          0.02111    0.01563     0.2904     0.04\n",
      "NOTE:      6    64     0.01         0.006473          0     0.2903     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01         0.02796   0.008929     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.003099          0     0.2903     0.05\n",
      "NOTE:      1    64     0.01         0.009509          0     0.2902     0.04\n",
      "NOTE:      2    64     0.01         0.003135          0     0.2902     0.04\n",
      "NOTE:      3    64     0.01         0.002859          0     0.2902     0.04\n",
      "NOTE:      4    64     0.01         0.006313          0     0.2901     0.04\n",
      "NOTE:      5    64     0.01         0.005851          0     0.2901     0.04\n",
      "NOTE:      6    64     0.01         0.005007          0       0.29     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01        0.005111          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.005445          0       0.29     0.05\n",
      "NOTE:      1    64     0.01         0.005349          0       0.29     0.04\n",
      "NOTE:      2    64     0.01          0.03973    0.01563     0.2899     0.04\n",
      "NOTE:      3    64     0.01         0.004484          0     0.2899     0.04\n",
      "NOTE:      4    64     0.01         0.005172          0     0.2898     0.04\n",
      "NOTE:      5    64     0.01         0.004457          0     0.2898     0.04\n",
      "NOTE:      6    64     0.01         0.007436          0     0.2897     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01          0.0103   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.004571          0     0.2897     0.05\n",
      "NOTE:      1    64     0.01         0.005433          0     0.2896     0.04\n",
      "NOTE:      2    64     0.01          0.02221    0.01563     0.2896     0.04\n",
      "NOTE:      3    64     0.01         0.005183          0     0.2895     0.04\n",
      "NOTE:      4    64     0.01          0.03421    0.01563     0.2895     0.04\n",
      "NOTE:      5    64     0.01         0.004009          0     0.2894     0.04\n",
      "NOTE:      6    64     0.01         0.003321          0     0.2894     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01         0.01128   0.004464     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.0232          0     0.2893     0.05\n",
      "NOTE:      1    64     0.01         0.003033          0     0.2893     0.04\n",
      "NOTE:      2    64     0.01         0.002198          0     0.2892     0.04\n",
      "NOTE:      3    64     0.01         0.004019          0     0.2892     0.04\n",
      "NOTE:      4    64     0.01         0.004275          0     0.2891     0.04\n",
      "NOTE:      5    64     0.01         0.002628          0     0.2891     0.04\n",
      "NOTE:      6    64     0.01         0.002822          0      0.289     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01        0.006025          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001698          0      0.289     0.05\n",
      "NOTE:      1    64     0.01         0.001945          0     0.2889     0.04\n",
      "NOTE:      2    64     0.01         0.003086          0     0.2889     0.04\n",
      "NOTE:      3    64     0.01         0.002964          0     0.2888     0.04\n",
      "NOTE:      4    64     0.01         0.003343          0     0.2888     0.04\n",
      "NOTE:      5    64     0.01         0.002399          0     0.2887     0.04\n",
      "NOTE:      6    64     0.01          0.03197    0.01563     0.2887     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6          0.01        0.006773   0.002232     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002136          0     0.2886     0.05\n",
      "NOTE:      1    64    0.001         0.002287          0     0.2886     0.04\n",
      "NOTE:      2    64    0.001         0.008953          0     0.2885     0.04\n",
      "NOTE:      3    64    0.001         0.009934          0     0.2885     0.04\n",
      "NOTE:      4    64    0.001         0.003731          0     0.2884     0.04\n",
      "NOTE:      5    64    0.001         0.003093          0     0.2884     0.04\n",
      "NOTE:      6    64    0.001         0.003537          0     0.2884     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001         0.00481          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002492          0     0.2884     0.05\n",
      "NOTE:      1    64    0.001         0.002677          0     0.2883     0.04\n",
      "NOTE:      2    64    0.001          0.00417          0     0.2883     0.04\n",
      "NOTE:      3    64    0.001          0.01629    0.01563     0.2883     0.04\n",
      "NOTE:      4    64    0.001         0.002416          0     0.2883     0.04\n",
      "NOTE:      5    64    0.001         0.002448          0     0.2882     0.04\n",
      "NOTE:      6    64    0.001         0.006092          0     0.2882     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001        0.005227   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.003616          0     0.2882     0.05\n",
      "NOTE:      1    64    0.001         0.006732          0     0.2882     0.04\n",
      "NOTE:      2    64    0.001         0.003747          0     0.2882     0.04\n",
      "NOTE:      3    64    0.001          0.00243          0     0.2882     0.04\n",
      "NOTE:      4    64    0.001         0.003257          0     0.2882     0.04\n",
      "NOTE:      5    64    0.001         0.002893          0     0.2881     0.04\n",
      "NOTE:      6    64    0.001         0.003773          0     0.2881     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001        0.003778          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.003264          0     0.2881     0.05\n",
      "NOTE:      1    64    0.001         0.002682          0     0.2881     0.04\n",
      "NOTE:      2    64    0.001         0.003184          0     0.2881     0.04\n",
      "NOTE:      3    64    0.001         0.002914          0     0.2881     0.04\n",
      "NOTE:      4    64    0.001         0.004279          0     0.2881     0.04\n",
      "NOTE:      5    64    0.001         0.002356          0     0.2881     0.04\n",
      "NOTE:      6    64    0.001         0.003236          0     0.2881     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001        0.003131          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002712          0     0.2881     0.05\n",
      "NOTE:      1    64    0.001         0.003857          0     0.2881     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      2    64    0.001           0.0279    0.01563      0.288     0.04\n",
      "NOTE:      3    64    0.001         0.004087          0      0.288     0.04\n",
      "NOTE:      4    64    0.001          0.00221          0      0.288     0.04\n",
      "NOTE:      5    64    0.001         0.003832          0      0.288     0.04\n",
      "NOTE:      6    64    0.001         0.002647          0      0.288     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001        0.006749   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002017          0      0.288     0.05\n",
      "NOTE:      1    64    0.001         0.001843          0      0.288     0.04\n",
      "NOTE:      2    64    0.001         0.002088          0      0.288     0.04\n",
      "NOTE:      3    64    0.001         0.001927          0      0.288     0.04\n",
      "NOTE:      4    64    0.001         0.002945          0      0.288     0.04\n",
      "NOTE:      5    64    0.001         0.003007          0      0.288     0.04\n",
      "NOTE:      6    64    0.001         0.002194          0      0.288     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001        0.002289          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.003167          0      0.288     0.05\n",
      "NOTE:      1    64    0.001         0.002354          0      0.288     0.04\n",
      "NOTE:      2    64    0.001         0.002627          0      0.288     0.04\n",
      "NOTE:      3    64    0.001         0.002101          0     0.2879     0.04\n",
      "NOTE:      4    64    0.001         0.007791          0     0.2879     0.04\n",
      "NOTE:      5    64    0.001         0.001921          0     0.2879     0.04\n",
      "NOTE:      6    64    0.001         0.001897          0     0.2879     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001        0.003123          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001668          0     0.2879     0.05\n",
      "NOTE:      1    64    0.001         0.001733          0     0.2879     0.04\n",
      "NOTE:      2    64    0.001         0.008361          0     0.2879     0.04\n",
      "NOTE:      3    64    0.001         0.002212          0     0.2879     0.04\n",
      "NOTE:      4    64    0.001         0.004167          0     0.2879     0.04\n",
      "NOTE:      5    64    0.001         0.002359          0     0.2879     0.04\n",
      "NOTE:      6    64    0.001         0.004545          0     0.2879     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14        0.001        0.003578          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001819          0     0.2879     0.05\n",
      "NOTE:      1    64    0.001         0.003411          0     0.2879     0.04\n",
      "NOTE:      2    64    0.001         0.003287          0     0.2879     0.04\n",
      "NOTE:      3    64    0.001         0.002655          0     0.2879     0.04\n",
      "NOTE:      4    64    0.001         0.001856          0     0.2879     0.04\n",
      "NOTE:      5    64    0.001         0.002785          0     0.2879     0.04\n",
      "NOTE:      6    64    0.001         0.002554          0     0.2878     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15        0.001        0.002624          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001899          0     0.2878     0.05\n",
      "NOTE:      1    64    0.001         0.003737          0     0.2878     0.04\n",
      "NOTE:      2    64    0.001         0.002546          0     0.2878     0.04\n",
      "NOTE:      3    64    0.001         0.003796          0     0.2878     0.04\n",
      "NOTE:      4    64    0.001         0.003074          0     0.2878     0.04\n",
      "NOTE:      5    64    0.001         0.002143          0     0.2878     0.04\n",
      "NOTE:      6    64    0.001         0.002025          0     0.2878     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16        0.001        0.002746          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001907          0     0.2878     0.05\n",
      "NOTE:      1    64   0.0001         0.005483          0     0.2878     0.04\n",
      "NOTE:      2    64   0.0001         0.003263          0     0.2878     0.04\n",
      "NOTE:      3    64   0.0001         0.002934          0     0.2878     0.04\n",
      "NOTE:      4    64   0.0001         0.002079          0     0.2878     0.04\n",
      "NOTE:      5    64   0.0001         0.001893          0     0.2878     0.04\n",
      "NOTE:      6    64   0.0001         0.001462          0     0.2878     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001        0.002717          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01775          0     0.2878     0.05\n",
      "NOTE:      1    64   0.0001         0.001609          0     0.2878     0.04\n",
      "NOTE:      2    64   0.0001         0.003058          0     0.2878     0.04\n",
      "NOTE:      3    64   0.0001         0.003015          0     0.2878     0.04\n",
      "NOTE:      4    64   0.0001         0.003291          0     0.2878     0.04\n",
      "NOTE:      5    64   0.0001         0.002314          0     0.2878     0.04\n",
      "NOTE:      6    64   0.0001         0.002655          0     0.2878     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0001        0.004813          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002199          0     0.2878     0.05\n",
      "NOTE:      1    64   0.0001         0.006033          0     0.2878     0.04\n",
      "NOTE:      2    64   0.0001         0.003357          0     0.2878     0.04\n",
      "NOTE:      3    64   0.0001         0.002554          0     0.2878     0.04\n",
      "NOTE:      4    64   0.0001          0.02827    0.01563     0.2878     0.04\n",
      "NOTE:      5    64   0.0001         0.001927          0     0.2878     0.04\n",
      "NOTE:      6    64   0.0001         0.002274          0     0.2878     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001         0.00666   0.002232     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007677          0     0.2878     0.05\n",
      "NOTE:      1    64   0.0001            0.002          0     0.2878     0.04\n",
      "NOTE:      2    64   0.0001          0.00265          0     0.2877     0.04\n",
      "NOTE:      3    64   0.0001         0.002856          0     0.2877     0.04\n",
      "NOTE:      4    64   0.0001         0.001838          0     0.2877     0.04\n",
      "NOTE:      5    64   0.0001         0.002676          0     0.2877     0.04\n",
      "NOTE:      6    64   0.0001         0.002228          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0001        0.003132          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.002363          0     0.2877     0.05\n",
      "NOTE:      1    64   0.0001         0.002158          0     0.2877     0.04\n",
      "NOTE:      2    64   0.0001         0.002831          0     0.2877     0.04\n",
      "NOTE:      3    64   0.0001         0.002721          0     0.2877     0.04\n",
      "NOTE:      4    64   0.0001         0.002686          0     0.2877     0.04\n",
      "NOTE:      5    64   0.0001         0.003024          0     0.2877     0.04\n",
      "NOTE:      6    64   0.0001         0.003927          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0001        0.002816          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.003166          0     0.2877     0.05\n",
      "NOTE:      1    64   0.0001         0.003363          0     0.2877     0.04\n",
      "NOTE:      2    64   0.0001          0.00383          0     0.2877     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64   0.0001         0.002211          0     0.2877     0.04\n",
      "NOTE:      4    64   0.0001         0.003209          0     0.2877     0.04\n",
      "NOTE:      5    64   0.0001         0.002548          0     0.2877     0.04\n",
      "NOTE:      6    64   0.0001          0.00309          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0001         0.00306          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.003144          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.001503          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.002351          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.001807          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001         0.003743          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.001947          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.002839          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5        0.002476          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001902          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.003328          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.002271          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.003071          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001          0.00178          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.004792          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.002441          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5        0.002798          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002376          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.002678          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.003392          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.003094          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001         0.001657          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.002743          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.002477          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5        0.002631          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.003375          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.002603          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.002234          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.002094          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001         0.002028          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.003029          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.004164          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5        0.002789          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002519          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.004752          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.002736          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.002349          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001         0.002941          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.002715          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.002871          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5        0.002983          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002131          0     0.2877     0.05\n",
      "NOTE:      1    64  0.00001         0.003734          0     0.2877     0.04\n",
      "NOTE:      2    64  0.00001         0.002564          0     0.2877     0.04\n",
      "NOTE:      3    64  0.00001         0.002601          0     0.2877     0.04\n",
      "NOTE:      4    64  0.00001         0.002997          0     0.2877     0.04\n",
      "NOTE:      5    64  0.00001         0.001566          0     0.2877     0.04\n",
      "NOTE:      6    64  0.00001         0.003136          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-5        0.002675          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002384          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.003003          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.004876          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6         0.003584          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.004017          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6          0.00166          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.002321          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-6        0.003121          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.005965          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.002171          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.002348          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6          0.00293          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6          0.00326          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.002396          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.003254          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-6        0.003189          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002214          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.001724          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.003645          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6         0.002102          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.001907          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.002537          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.004047          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6        0.002597          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001712          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.002235          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.001482          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6         0.003168          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.002836          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.002477          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.001799          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6        0.002244          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.0022          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.001364          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.002083          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6          0.00278          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.003756          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6          0.01694          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.002971          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6        0.004584          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002182          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.002912          0     0.2877     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      2    64     1E-6         0.001861          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6         0.001961          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.004217          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.003583          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6          0.00216          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6        0.002696          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002219          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.001796          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.001739          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6          0.00271          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.002302          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.001864          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.003544          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-6        0.002311          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.005323          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-6         0.003265          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-6         0.003347          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-6         0.009209          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-6         0.003452          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-6         0.002173          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-6         0.004903          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-6        0.004525          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.003392          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.002017          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7          0.01643          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.003826          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.002678          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.002818          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.009391          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-7        0.005793          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002779          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.003279          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7         0.002601          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.001833          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.001915          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.002703          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.002378          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7        0.002498          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.008456          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.002878          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7         0.002122          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.002091          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.002469          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.002182          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.002241          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7        0.003205          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.01517          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.002117          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7         0.003904          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.002538          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.002682          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.002285          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.004344          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7         0.00472          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002552          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.002249          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7         0.003557          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.003932          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.003624          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.001521          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.001769          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7        0.002743          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.002473          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-7         0.002365          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-7         0.003585          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-7         0.002403          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-7         0.002939          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-7         0.002159          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-7         0.001468          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-7        0.002485          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.003622          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.003261          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8          0.00246          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8          0.00409          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-8          0.00216          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8          0.01321          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.002634          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-8         0.00449          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01605          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.001679          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8         0.003229          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8         0.002554          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-8         0.002161          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8         0.002138          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.002057          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8        0.004267          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001728          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.002391          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8         0.002554          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8         0.002061          0     0.2877     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4    64     1E-8         0.008139          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8         0.003032          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.001974          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8        0.003126          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001689          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.002364          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8         0.001991          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8         0.002338          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-8         0.003557          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8         0.003567          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.006266          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8         0.00311          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001586          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.002489          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8         0.002709          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8         0.004163          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-8         0.003878          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8         0.003309          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.002219          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8        0.002908          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.002389          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-8         0.002453          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-8         0.001788          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-8         0.003394          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-8         0.002504          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-8         0.002684          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-8         0.002467          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8        0.002526          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9          0.00199          0     0.2877     0.05\n",
      "NOTE:      1    64     1E-9         0.002179          0     0.2877     0.04\n",
      "NOTE:      2    64     1E-9          0.00256          0     0.2877     0.04\n",
      "NOTE:      3    64     1E-9         0.002102          0     0.2877     0.04\n",
      "NOTE:      4    64     1E-9         0.007334          0     0.2877     0.04\n",
      "NOTE:      5    64     1E-9         0.002038          0     0.2877     0.04\n",
      "NOTE:      6    64     1E-9         0.001782          0     0.2877     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-9        0.002855          0     0.30\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      15.46 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_n3w4tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.018488</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.027955</td>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.290419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.289857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.289520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.006025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.288817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>63</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.288283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.288093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>67</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.288037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>68</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>70</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>71</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>72</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>73</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>74</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>75</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.287756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>77</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>78</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>79</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>80</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>81</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>82</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>83</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>84</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>85</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>86</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>87</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>88</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>89</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>90</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>91</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>92</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>93</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>94</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>95</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>96</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>97</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>98</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>99</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>101</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>102</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>103</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>104</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>105</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(ethem-kinginthenorth)</td>\n",
       "      <td>Model_N3w4tJ_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_N3w4tJ_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 23s</span> &#183; <span class=\"cas-user\">user 19.6s</span> &#183; <span class=\"cas-sys\">sys 4.01s</span> &#183; <span class=\"cas-memory\">mem 131MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_n3w4tj\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      56  1.000000e-02  0.018488  0.006696  0.290600\n",
       " 1      57  1.000000e-02  0.027955  0.008929  0.290419\n",
       " 2      58  1.000000e-02  0.005111  0.000000  0.290164\n",
       " 3      59  1.000000e-02  0.010297  0.002232  0.289857\n",
       " 4      60  1.000000e-02  0.011276  0.004464  0.289520\n",
       " 5      61  1.000000e-02  0.006025  0.000000  0.289173\n",
       " 6      62  1.000000e-02  0.006773  0.002232  0.288817\n",
       " 7      63  1.000000e-03  0.004810  0.000000  0.288484\n",
       " 8      64  1.000000e-03  0.005227  0.002232  0.288283\n",
       " 9      65  1.000000e-03  0.003778  0.000000  0.288168\n",
       " 10     66  1.000000e-03  0.003131  0.000000  0.288093\n",
       " 11     67  1.000000e-03  0.006749  0.002232  0.288037\n",
       " 12     68  1.000000e-03  0.002289  0.000000  0.287990\n",
       " 13     69  1.000000e-03  0.003123  0.000000  0.287947\n",
       " 14     70  1.000000e-03  0.003578  0.000000  0.287906\n",
       " 15     71  1.000000e-03  0.002624  0.000000  0.287866\n",
       " 16     72  1.000000e-03  0.002746  0.000000  0.287826\n",
       " 17     73  1.000000e-04  0.002717  0.000000  0.287790\n",
       " 18     74  1.000000e-04  0.004813  0.000000  0.287769\n",
       " 19     75  1.000000e-04  0.006660  0.002232  0.287756\n",
       " 20     76  1.000000e-04  0.003132  0.000000  0.287748\n",
       " 21     77  1.000000e-04  0.002816  0.000000  0.287742\n",
       " 22     78  1.000000e-04  0.003060  0.000000  0.287737\n",
       " 23     79  1.000000e-05  0.002476  0.000000  0.287733\n",
       " 24     80  1.000000e-05  0.002798  0.000000  0.287731\n",
       " 25     81  1.000000e-05  0.002631  0.000000  0.287730\n",
       " 26     82  1.000000e-05  0.002789  0.000000  0.287729\n",
       " 27     83  1.000000e-05  0.002983  0.000000  0.287728\n",
       " 28     84  1.000000e-05  0.002675  0.000000  0.287728\n",
       " 29     85  1.000000e-06  0.003121  0.000000  0.287727\n",
       " 30     86  1.000000e-06  0.003189  0.000000  0.287727\n",
       " 31     87  1.000000e-06  0.002597  0.000000  0.287727\n",
       " 32     88  1.000000e-06  0.002244  0.000000  0.287727\n",
       " 33     89  1.000000e-06  0.004584  0.000000  0.287727\n",
       " 34     90  1.000000e-06  0.002696  0.000000  0.287727\n",
       " 35     91  1.000000e-06  0.002311  0.000000  0.287727\n",
       " 36     92  1.000000e-06  0.004525  0.000000  0.287727\n",
       " 37     93  1.000000e-07  0.005793  0.000000  0.287727\n",
       " 38     94  1.000000e-07  0.002498  0.000000  0.287727\n",
       " 39     95  1.000000e-07  0.003205  0.000000  0.287727\n",
       " 40     96  1.000000e-07  0.004720  0.000000  0.287727\n",
       " 41     97  1.000000e-07  0.002743  0.000000  0.287727\n",
       " 42     98  1.000000e-07  0.002485  0.000000  0.287727\n",
       " 43     99  1.000000e-08  0.004490  0.000000  0.287727\n",
       " 44    100  1.000000e-08  0.004267  0.000000  0.287727\n",
       " 45    101  1.000000e-08  0.003126  0.000000  0.287727\n",
       " 46    102  1.000000e-08  0.003110  0.000000  0.287727\n",
       " 47    103  1.000000e-08  0.002908  0.000000  0.287727\n",
       " 48    104  1.000000e-08  0.002526  0.000000  0.287727\n",
       " 49    105  1.000000e-09  0.002855  0.000000  0.287727\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(ethem-kinginthenorth)  Model_N3w4tJ_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_N3w4tJ_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 23s, user: 19.6s, sys: 4.01s, mem: 131mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Learning Rate Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a flexible approach to define your learning rate policy using FCMP function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00318s</span> &#183; <span class=\"cas-user\">user 0.00111s</span> &#183; <span class=\"cas-sys\">sys 0.00204s</span> &#183; <span class=\"cas-memory\">mem 3.31MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.00318s, user: 0.00111s, sys: 0.00204s, mem: 3.31mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cool_down_iters = 5\n",
    "patience = 1\n",
    "sess.addRoutines(\n",
    "            routineCode = '''\n",
    "                        function reduce_lr_on_plateau(rate, initRate, gamma, loss[*]);\n",
    "                            len = dim(loss);\n",
    "                            temp_rate = initRate;\n",
    "                            cool_down_counter = {0};\n",
    "                            best = loss[1];\n",
    "                            do i=1 to len;\n",
    "                    \n",
    "                                if loss[i] < best then do;\n",
    "                                    best = loss[i];\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                                else bad_epoch = bad_epoch + 1;\n",
    "                    \n",
    "                                if cool_down_counter > 0 then do;\n",
    "                                    cool_down_counter = cool_down_counter - 1;\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                    \n",
    "                                if bad_epoch > {1} then do;\n",
    "                                    temp_rate = temp_rate * gamma;\n",
    "                                    cool_down_counter = {0};\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                            end;\n",
    "                            rate = temp_rate;\n",
    "                            put rate=;\n",
    "                            return(rate);\n",
    "                        endsub;\n",
    "                        '''.format(cool_down_iters, patience),\n",
    "            package = 'pkg',\n",
    "            funcTable = dict(name = 'reduce_lr_on_plateau', replace = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your FCMP function name in fcmp_learning_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = FCMPLR(conn=sess, fcmp_learning_rate='reduce_lr_on_plateau',\n",
    "                      learning_rate = 0.01, gamma = 0.1)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=16, log_level=3, max_epochs=50, reg_l2=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "WARNING: Failed to load TKGPU extension.\n",
      "NOTE: Using dlgrd008.unx.sas.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       6.90 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002573          0     0.2877     0.31\n",
      "NOTE:      1    64     0.01         0.001984          0     0.2877     0.04\n",
      "NOTE:      2    64     0.01         0.005509          0     0.2877     0.04\n",
      "NOTE:      3    64     0.01         0.003904          0     0.2877     0.04\n",
      "NOTE:      4    64     0.01         0.003093          0     0.2877     0.04\n",
      "NOTE:      5    64     0.01         0.001971          0     0.2877     0.04\n",
      "NOTE:      6    64     0.01         0.003455          0     0.2876     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01        0.003213          0     0.57\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.00257          0     0.2876     0.05\n",
      "NOTE:      1    64     0.01         0.002591          0     0.2876     0.04\n",
      "NOTE:      2    64     0.01          0.00383          0     0.2875     0.04\n",
      "NOTE:      3    64     0.01         0.002159          0     0.2875     0.04\n",
      "NOTE:      4    64     0.01         0.002937          0     0.2875     0.04\n",
      "NOTE:      5    64     0.01         0.002741          0     0.2874     0.04\n",
      "NOTE:      6    64     0.01         0.002528          0     0.2874     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01        0.002765          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002434          0     0.2873     0.05\n",
      "NOTE:      1    64     0.01         0.001488          0     0.2873     0.04\n",
      "NOTE:      2    64     0.01          0.00266          0     0.2872     0.04\n",
      "NOTE:      3    64     0.01         0.001743          0     0.2872     0.04\n",
      "NOTE:      4    64     0.01         0.001906          0     0.2871     0.04\n",
      "NOTE:      5    64     0.01         0.002594          0     0.2871     0.04\n",
      "NOTE:      6    64     0.01          0.00205          0     0.2871     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01        0.002125          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002735          0      0.287     0.05\n",
      "NOTE:      1    64     0.01         0.001564          0     0.2869     0.04\n",
      "NOTE:      2    64     0.01         0.001359          0     0.2869     0.04\n",
      "NOTE:      3    64     0.01         0.001676          0     0.2868     0.04\n",
      "NOTE:      4    64     0.01         0.002227          0     0.2868     0.04\n",
      "NOTE:      5    64     0.01         0.002059          0     0.2867     0.04\n",
      "NOTE:      6    64     0.01          0.00271          0     0.2867     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01        0.002047          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001651          0     0.2866     0.05\n",
      "NOTE:      1    64     0.01         0.001794          0     0.2866     0.04\n",
      "NOTE:      2    64     0.01         0.001849          0     0.2865     0.04\n",
      "NOTE:      3    64     0.01         0.002758          0     0.2865     0.04\n",
      "NOTE:      4    64     0.01         0.002277          0     0.2864     0.04\n",
      "NOTE:      5    64     0.01         0.001434          0     0.2864     0.04\n",
      "NOTE:      6    64     0.01         0.002395          0     0.2863     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01        0.002022          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.00183          0     0.2863     0.05\n",
      "NOTE:      1    64     0.01         0.001624          0     0.2862     0.04\n",
      "NOTE:      2    64     0.01         0.002095          0     0.2861     0.04\n",
      "NOTE:      3    64     0.01         0.002391          0     0.2861     0.04\n",
      "NOTE:      4    64     0.01         0.002016          0      0.286     0.04\n",
      "NOTE:      5    64     0.01         0.002636          0      0.286     0.04\n",
      "NOTE:      6    64     0.01         0.001814          0     0.2859     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01        0.002058          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001595          0     0.2859     0.05\n",
      "NOTE:      1    64     0.01         0.001659          0     0.2858     0.04\n",
      "NOTE:      2    64     0.01         0.001566          0     0.2858     0.04\n",
      "NOTE:      3    64     0.01         0.002169          0     0.2857     0.04\n",
      "NOTE:      4    64     0.01           0.0018          0     0.2856     0.04\n",
      "NOTE:      5    64     0.01         0.001888          0     0.2856     0.04\n",
      "NOTE:      6    64     0.01         0.001974          0     0.2855     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6          0.01        0.001807          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.002103          0     0.2855     0.05\n",
      "NOTE:      1    64     0.01         0.001935          0     0.2854     0.04\n",
      "NOTE:      2    64     0.01         0.002953          0     0.2854     0.04\n",
      "NOTE:      3    64     0.01          0.00169          0     0.2853     0.04\n",
      "NOTE:      4    64     0.01         0.002085          0     0.2853     0.04\n",
      "NOTE:      5    64     0.01         0.002566          0     0.2852     0.04\n",
      "NOTE:      6    64     0.01         0.001774          0     0.2851     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7          0.01        0.002158          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.001479          0     0.2851     0.05\n",
      "NOTE:      1    64     0.01         0.001687          0      0.285     0.04\n",
      "NOTE:      2    64     0.01         0.001441          0      0.285     0.04\n",
      "NOTE:      3    64     0.01          0.00199          0     0.2849     0.04\n",
      "NOTE:      4    64     0.01         0.002029          0     0.2849     0.04\n",
      "NOTE:      5    64     0.01         0.003119          0     0.2848     0.04\n",
      "NOTE:      6    64     0.01         0.002447          0     0.2848     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8          0.01        0.002028          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001042          0     0.2847     0.05\n",
      "NOTE:      1    64    0.001         0.001377          0     0.2847     0.04\n",
      "NOTE:      2    64    0.001         0.001137          0     0.2846     0.04\n",
      "NOTE:      3    64    0.001         0.001692          0     0.2846     0.04\n",
      "NOTE:      4    64    0.001         0.003105          0     0.2845     0.04\n",
      "NOTE:      5    64    0.001         0.002261          0     0.2845     0.04\n",
      "NOTE:      6    64    0.001         0.001136          0     0.2845     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001        0.001678          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001551          0     0.2844     0.05\n",
      "NOTE:      1    64    0.001         0.001879          0     0.2844     0.04\n",
      "NOTE:      2    64    0.001         0.001124          0     0.2844     0.04\n",
      "NOTE:      3    64    0.001         0.001758          0     0.2844     0.04\n",
      "NOTE:      4    64    0.001         0.001683          0     0.2843     0.04\n",
      "NOTE:      5    64    0.001         0.002884          0     0.2843     0.04\n",
      "NOTE:      6    64    0.001         0.001776          0     0.2843     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001        0.001808          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001376          0     0.2843     0.05\n",
      "NOTE:      1    64    0.001         0.002033          0     0.2843     0.04\n",
      "NOTE:      2    64    0.001         0.001629          0     0.2842     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64    0.001         0.001127          0     0.2842     0.04\n",
      "NOTE:      4    64    0.001         0.001463          0     0.2842     0.04\n",
      "NOTE:      5    64    0.001         0.001893          0     0.2842     0.04\n",
      "NOTE:      6    64    0.001         0.001497          0     0.2842     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001        0.001574          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.002333          0     0.2842     0.05\n",
      "NOTE:      1    64    0.001         0.001009          0     0.2842     0.04\n",
      "NOTE:      2    64    0.001         0.001719          0     0.2842     0.04\n",
      "NOTE:      3    64    0.001          0.00205          0     0.2842     0.04\n",
      "NOTE:      4    64    0.001         0.001507          0     0.2841     0.04\n",
      "NOTE:      5    64    0.001         0.001696          0     0.2841     0.04\n",
      "NOTE:      6    64    0.001         0.001148          0     0.2841     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001        0.001637          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.004627          0     0.2841     0.05\n",
      "NOTE:      1    64    0.001         0.001266          0     0.2841     0.04\n",
      "NOTE:      2    64    0.001         0.001779          0     0.2841     0.04\n",
      "NOTE:      3    64    0.001         0.001311          0     0.2841     0.04\n",
      "NOTE:      4    64    0.001         0.001966          0     0.2841     0.04\n",
      "NOTE:      5    64    0.001         0.001675          0     0.2841     0.04\n",
      "NOTE:      6    64    0.001         0.001489          0     0.2841     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001        0.002016          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.00141          0     0.2841     0.05\n",
      "NOTE:      1    64    0.001         0.001231          0     0.2841     0.04\n",
      "NOTE:      2    64    0.001         0.002417          0     0.2841     0.04\n",
      "NOTE:      3    64    0.001         0.002065          0      0.284     0.04\n",
      "NOTE:      4    64    0.001         0.001112          0      0.284     0.04\n",
      "NOTE:      5    64    0.001         0.001405          0      0.284     0.04\n",
      "NOTE:      6    64    0.001         0.001412          0      0.284     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14        0.001        0.001579          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.001281          0      0.284     0.05\n",
      "NOTE:      1    64    0.001         0.001613          0      0.284     0.04\n",
      "NOTE:      2    64    0.001         0.001807          0      0.284     0.04\n",
      "NOTE:      3    64    0.001         0.001115          0      0.284     0.04\n",
      "NOTE:      4    64    0.001         0.001679          0      0.284     0.04\n",
      "NOTE:      5    64    0.001         0.001984          0      0.284     0.04\n",
      "NOTE:      6    64    0.001         0.002036          0      0.284     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15        0.001        0.001645          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001361          0      0.284     0.05\n",
      "NOTE:      1    64   0.0001        0.0009823          0      0.284     0.04\n",
      "NOTE:      2    64   0.0001         0.002008          0      0.284     0.04\n",
      "NOTE:      3    64   0.0001         0.001541          0      0.284     0.04\n",
      "NOTE:      4    64   0.0001         0.001093          0      0.284     0.04\n",
      "NOTE:      5    64   0.0001         0.001494          0      0.284     0.04\n",
      "NOTE:      6    64   0.0001         0.001672          0      0.284     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001         0.00145          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001766          0      0.284     0.05\n",
      "NOTE:      1    64   0.0001         0.001307          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001         0.001877          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.001409          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001          0.00222          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001         0.001782          0     0.2839     0.04\n",
      "NOTE:      6    64   0.0001         0.001564          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001        0.001704          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001013          0     0.2839     0.05\n",
      "NOTE:      1    64   0.0001         0.001931          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001          0.00155          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.001623          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001         0.001046          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001         0.001278          0     0.2839     0.04\n",
      "NOTE:      6    64   0.0001         0.001027          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0001        0.001353          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001439          0     0.2839     0.05\n",
      "NOTE:      1    64   0.0001         0.001474          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001         0.002157          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.001422          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001         0.001481          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001        0.0009045          0     0.2839     0.04\n",
      "NOTE:      6    64   0.0001         0.001944          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001        0.001546          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001087          0     0.2839     0.05\n",
      "NOTE:      1    64   0.0001         0.001238          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001         0.002333          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.002584          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001         0.001103          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001          0.00163          0     0.2839     0.04\n",
      "NOTE:      6    64   0.0001         0.001639          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0001        0.001659          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.001231          0     0.2839     0.05\n",
      "NOTE:      1    64   0.0001         0.002267          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001         0.001638          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.001814          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001         0.002715          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001         0.001645          0     0.2839     0.04\n",
      "NOTE:      6    64   0.0001         0.001886          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0001        0.001885          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.00128          0     0.2839     0.05\n",
      "NOTE:      1    64   0.0001         0.002716          0     0.2839     0.04\n",
      "NOTE:      2    64   0.0001         0.006252          0     0.2839     0.04\n",
      "NOTE:      3    64   0.0001         0.001792          0     0.2839     0.04\n",
      "NOTE:      4    64   0.0001         0.001408          0     0.2839     0.04\n",
      "NOTE:      5    64   0.0001         0.001046          0     0.2839     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      6    64   0.0001         0.001451          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0001        0.002278          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001324          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.002333          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.002243          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001         0.001762          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.001636          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001        0.0009537          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.002356          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5        0.001801          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.001469          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.001457          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.001003          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001          0.00124          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.001216          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.002497          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.001983          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5        0.001552          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002405          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.001537          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.001153          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001         0.001933          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.002062          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.001424          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.001326          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5        0.001691          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002288          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.001711          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.001425          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001         0.001712          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.001007          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.001856          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.003774          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5        0.001968          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.00132          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.002136          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.001472          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001         0.002293          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.003676          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.001355          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001          0.00189          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5         0.00202          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.00187          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.001753          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001          0.00123          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001         0.002057          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001        0.0009641          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.001912          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.001952          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-5        0.001677          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.002765          0     0.2839     0.05\n",
      "NOTE:      1    64  0.00001         0.002279          0     0.2839     0.04\n",
      "NOTE:      2    64  0.00001         0.001778          0     0.2839     0.04\n",
      "NOTE:      3    64  0.00001        0.0009717          0     0.2839     0.04\n",
      "NOTE:      4    64  0.00001         0.001121          0     0.2839     0.04\n",
      "NOTE:      5    64  0.00001         0.001833          0     0.2839     0.04\n",
      "NOTE:      6    64  0.00001         0.001222          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-5         0.00171          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001746          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6        0.0009652          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001092          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.007236          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.001513          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.001858          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.002652          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-6        0.002438          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001601          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6         0.001578          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001027          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.001484          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.001073          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.001154          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6          0.00122          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6        0.001305          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.01093          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6         0.002031          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001575          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.001709          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.002474          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.001403          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.001447          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6        0.003081          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001486          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6         0.001274          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001826          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.001328          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.001027          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.001628          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.001555          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6        0.001446          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002384          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6         0.002407          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001674          0     0.2839     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3    64     1E-6         0.001079          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.002927          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.003049          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.001197          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6        0.002103          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.002999          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6         0.002152          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001679          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.001789          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.001293          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.001856          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.001777          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-6        0.001935          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.001852          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-6        0.0009738          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-6         0.001617          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-6         0.002958          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-6         0.001612          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-6         0.002093          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-6         0.002111          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-6        0.001888          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001087          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001864          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.001345          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7          0.00523          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.001836          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001428          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7         0.001202          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-7        0.001999          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001126          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-7          0.00167          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.002723          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001384          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.002168          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001641          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7         0.001376          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7        0.001727          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.00261          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001114          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.001116          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001351          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.002415          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001516          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7         0.001287          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7         0.00163          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.07052    0.03125     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001416          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7          0.00113          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001794          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.001421          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001815          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7          0.00117          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7         0.01132   0.004464     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7            0.026    0.01563     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001591          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.001184          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001454          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.001173          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001815          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7        0.0008586          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7        0.004869   0.002232     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001592          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001598          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.002126          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001541          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.001791          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.001427          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7          0.00184          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-7        0.001702          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.001696          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-7         0.001787          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-7         0.001945          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-7         0.001131          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-7         0.001468          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-7         0.007991          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-7         0.001571          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-7        0.002513          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001198          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8           0.0013          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.002897          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.001637          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.001067          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8         0.001886          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8         0.002002          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8        0.001712          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.002249          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8        0.0009804          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.001963          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.001061          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.002318          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8         0.001661          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8         0.001884          0     0.2839     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8        0.001731          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.001448          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8         0.001468          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.002593          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.001386          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.001744          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8         0.001393          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8        0.0009173          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8        0.001564          0     0.31\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.000992          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8         0.002778          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.001708          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.002592          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.002579          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8          0.00153          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8          0.00139          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8        0.001938          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.002366          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8         0.001679          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.002154          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.001312          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.001637          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8         0.001142          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8         0.001454          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8        0.001678          0     0.30\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.00116          0     0.2839     0.05\n",
      "NOTE:      1    64     1E-8         0.002079          0     0.2839     0.04\n",
      "NOTE:      2    64     1E-8         0.002769          0     0.2839     0.04\n",
      "NOTE:      3    64     1E-8         0.001361          0     0.2839     0.04\n",
      "NOTE:      4    64     1E-8         0.001865          0     0.2839     0.04\n",
      "NOTE:      5    64     1E-8         0.002362          0     0.2839     0.04\n",
      "NOTE:      6    64     1E-8         0.002229          0     0.2839     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-8        0.001975          0     0.30\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      15.51 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_n3w4tj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.003213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>111</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>112</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>113</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>114</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>115</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>116</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>117</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>118</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>119</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>120</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>121</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>122</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>123</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>124</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>125</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>126</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>127</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>128</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>129</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>130</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>131</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>132</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>133</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>134</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>135</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>136</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>137</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>138</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>139</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>140</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>141</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>142</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>143</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>144</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>145</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>146</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.011324</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>147</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>148</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>149</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>150</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>151</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>152</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>153</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>154</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>155</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(ethem-kinginthenorth)</td>\n",
       "      <td>Model_N3w4tJ_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_N3w4tJ_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 23s</span> &#183; <span class=\"cas-user\">user 19.4s</span> &#183; <span class=\"cas-sys\">sys 4.11s</span> &#183; <span class=\"cas-memory\">mem 131MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_n3w4tj\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0     106  1.000000e-02  0.003213  0.000000  0.287687\n",
       " 1     107  1.000000e-02  0.002765  0.000000  0.287492\n",
       " 2     108  1.000000e-02  0.002125  0.000000  0.287194\n",
       " 3     109  1.000000e-02  0.002047  0.000000  0.286846\n",
       " 4     110  1.000000e-02  0.002022  0.000000  0.286475\n",
       " 5     111  1.000000e-02  0.002058  0.000000  0.286092\n",
       " 6     112  1.000000e-02  0.001807  0.000000  0.285705\n",
       " 7     113  1.000000e-02  0.002158  0.000000  0.285315\n",
       " 8     114  1.000000e-02  0.002028  0.000000  0.284925\n",
       " 9     115  1.000000e-03  0.001678  0.000000  0.284570\n",
       " 10    116  1.000000e-03  0.001808  0.000000  0.284355\n",
       " 11    117  1.000000e-03  0.001574  0.000000  0.284232\n",
       " 12    118  1.000000e-03  0.001637  0.000000  0.284153\n",
       " 13    119  1.000000e-03  0.002016  0.000000  0.284095\n",
       " 14    120  1.000000e-03  0.001579  0.000000  0.284046\n",
       " 15    121  1.000000e-03  0.001645  0.000000  0.284003\n",
       " 16    122  1.000000e-04  0.001450  0.000000  0.283965\n",
       " 17    123  1.000000e-04  0.001704  0.000000  0.283943\n",
       " 18    124  1.000000e-04  0.001353  0.000000  0.283930\n",
       " 19    125  1.000000e-04  0.001546  0.000000  0.283922\n",
       " 20    126  1.000000e-04  0.001659  0.000000  0.283916\n",
       " 21    127  1.000000e-04  0.001885  0.000000  0.283911\n",
       " 22    128  1.000000e-04  0.002278  0.000000  0.283907\n",
       " 23    129  1.000000e-05  0.001801  0.000000  0.283903\n",
       " 24    130  1.000000e-05  0.001552  0.000000  0.283901\n",
       " 25    131  1.000000e-05  0.001691  0.000000  0.283899\n",
       " 26    132  1.000000e-05  0.001968  0.000000  0.283898\n",
       " 27    133  1.000000e-05  0.002020  0.000000  0.283898\n",
       " 28    134  1.000000e-05  0.001677  0.000000  0.283897\n",
       " 29    135  1.000000e-05  0.001710  0.000000  0.283897\n",
       " 30    136  1.000000e-06  0.002438  0.000000  0.283897\n",
       " 31    137  1.000000e-06  0.001305  0.000000  0.283896\n",
       " 32    138  1.000000e-06  0.003081  0.000000  0.283896\n",
       " 33    139  1.000000e-06  0.001446  0.000000  0.283896\n",
       " 34    140  1.000000e-06  0.002103  0.000000  0.283896\n",
       " 35    141  1.000000e-06  0.001935  0.000000  0.283896\n",
       " 36    142  1.000000e-06  0.001888  0.000000  0.283896\n",
       " 37    143  1.000000e-07  0.001999  0.000000  0.283896\n",
       " 38    144  1.000000e-07  0.001727  0.000000  0.283896\n",
       " 39    145  1.000000e-07  0.001630  0.000000  0.283896\n",
       " 40    146  1.000000e-07  0.011324  0.004464  0.283896\n",
       " 41    147  1.000000e-07  0.004869  0.002232  0.283896\n",
       " 42    148  1.000000e-07  0.001702  0.000000  0.283896\n",
       " 43    149  1.000000e-07  0.002513  0.000000  0.283896\n",
       " 44    150  1.000000e-08  0.001712  0.000000  0.283896\n",
       " 45    151  1.000000e-08  0.001731  0.000000  0.283896\n",
       " 46    152  1.000000e-08  0.001564  0.000000  0.283896\n",
       " 47    153  1.000000e-08  0.001938  0.000000  0.283896\n",
       " 48    154  1.000000e-08  0.001678  0.000000  0.283896\n",
       " 49    155  1.000000e-08  0.001975  0.000000  0.283896\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(ethem-kinginthenorth)  Model_N3w4tJ_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_N3w4tJ_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 23s, user: 19.4s, sys: 4.11s, mem: 131mb"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 9e-05s</span> &#183; <span class=\"cas-memory\">mem 0.195MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 9e-05s, mem: 0.195mb"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.endsession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
